{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import ast\n",
    "import heapq\n",
    "from copy import deepcopy\n",
    "from math import log\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import pkbar\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skimage import io\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import FreqDist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "unknown_token = '<unk>'\n",
    "padding_token = '<pad>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/styles_tags_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1163</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Sahara Team India Fanwear Round Neck Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'sahara', 'team', 'india',...</td>\n",
       "      <td>1163.jpg</td>\n",
       "      <td>['nike', 'team', 'india', 'round', 'neck', 'je...</td>\n",
       "      <td>[2.3143535e-07, 2.3702476e-09, 2.986898e-07, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Men Blue T20 Indian Cricket Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'men', 'blue', 'indian', '...</td>\n",
       "      <td>1164.jpg</td>\n",
       "      <td>['nike', 'men', 'blue', 'indian', 'cricket', '...</td>\n",
       "      <td>[2.2555818e-07, 1.8278894e-09, 7.16435e-09, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1165</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Mean Team India Cricket Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'mean', 'team', 'india', '...</td>\n",
       "      <td>1165.jpg</td>\n",
       "      <td>['nike', 'team', 'india', 'cricket', 'jersey']</td>\n",
       "      <td>[0.00035982736, 2.8710444e-06, 5.1504063e-05, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1525</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Puma Deck Navy Blue Backpack</td>\n",
       "      <td>['&lt;start&gt;', 'puma', 'deck', 'navy', 'blue', 'b...</td>\n",
       "      <td>1525.jpg</td>\n",
       "      <td>['puma', 'navy', 'blue', 'backpack']</td>\n",
       "      <td>[1.9060193e-05, 8.293974e-09, 9.6082715e-08, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1526</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>Black</td>\n",
       "      <td>Puma Big Cat Backpack Black</td>\n",
       "      <td>['&lt;start&gt;', 'puma', 'big', 'cat', 'backpack', ...</td>\n",
       "      <td>1526.jpg</td>\n",
       "      <td>['puma', 'big', 'cat', 'backpack', 'black']</td>\n",
       "      <td>[0.0015358106, 1.2226741e-07, 7.6661183e-07, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id masterCategory subCategory articleType baseColour  \\\n",
       "0  1163        Apparel     Topwear     Tshirts       Blue   \n",
       "1  1164        Apparel     Topwear     Tshirts       Blue   \n",
       "2  1165        Apparel     Topwear     Tshirts       Blue   \n",
       "3  1525    Accessories        Bags   Backpacks  Navy Blue   \n",
       "4  1526    Accessories        Bags   Backpacks      Black   \n",
       "\n",
       "                                              title  \\\n",
       "0  Nike Sahara Team India Fanwear Round Neck Jersey   \n",
       "1           Nike Men Blue T20 Indian Cricket Jersey   \n",
       "2               Nike Mean Team India Cricket Jersey   \n",
       "3                      Puma Deck Navy Blue Backpack   \n",
       "4                       Puma Big Cat Backpack Black   \n",
       "\n",
       "                                           tokenized file_name  \\\n",
       "0  ['<start>', 'nike', 'sahara', 'team', 'india',...  1163.jpg   \n",
       "1  ['<start>', 'nike', 'men', 'blue', 'indian', '...  1164.jpg   \n",
       "2  ['<start>', 'nike', 'mean', 'team', 'india', '...  1165.jpg   \n",
       "3  ['<start>', 'puma', 'deck', 'navy', 'blue', 'b...  1525.jpg   \n",
       "4  ['<start>', 'puma', 'big', 'cat', 'backpack', ...  1526.jpg   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['nike', 'team', 'india', 'round', 'neck', 'je...   \n",
       "1  ['nike', 'men', 'blue', 'indian', 'cricket', '...   \n",
       "2     ['nike', 'team', 'india', 'cricket', 'jersey']   \n",
       "3               ['puma', 'navy', 'blue', 'backpack']   \n",
       "4        ['puma', 'big', 'cat', 'backpack', 'black']   \n",
       "\n",
       "                                           tags_pred  \n",
       "0  [2.3143535e-07, 2.3702476e-09, 2.986898e-07, 3...  \n",
       "1  [2.2555818e-07, 1.8278894e-09, 7.16435e-09, 3....  \n",
       "2  [0.00035982736, 2.8710444e-06, 5.1504063e-05, ...  \n",
       "3  [1.9060193e-05, 8.293974e-09, 9.6082715e-08, 2...  \n",
       "4  [0.0015358106, 1.2226741e-07, 7.6661183e-07, 9...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masterCategory</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accessories</th>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apparel</th>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21389</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footwear</th>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Free Items</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Care</th>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2395</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sporting Goods</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  subCategory  articleType  baseColour  title  tokenized  \\\n",
       "masterCategory                                                                  \n",
       "Accessories     11286        11286        11286       11286  11286      11286   \n",
       "Apparel         21395        21395        21395       21389  21395      21395   \n",
       "Footwear         9220         9220         9220        9220   9220       9220   \n",
       "Free Items        105          105          105         105    105        105   \n",
       "Home                1            1            1           1      1          1   \n",
       "Personal Care    2399         2399         2399        2395   2399       2399   \n",
       "Sporting Goods     25           25           25          25     25         25   \n",
       "\n",
       "                file_name   tags  tags_pred  \n",
       "masterCategory                               \n",
       "Accessories         11286  11286      11286  \n",
       "Apparel             21395  21395      21395  \n",
       "Footwear             9220   9220       9220  \n",
       "Free Items            105    105        105  \n",
       "Home                    1      1          1  \n",
       "Personal Care        2399   2399       2399  \n",
       "Sporting Goods         25     25         25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"masterCategory\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_master_category = [\n",
    "    \"Home\",\n",
    "    \"Sporting Goods\",\n",
    "    \"Free Items\"\n",
    "]\n",
    "\n",
    "df = df[~df.masterCategory.isin(removed_master_category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stringified-list-type columns back to list\n",
    "df.tokenized = df.tokenized.apply(lambda tokens: ast.literal_eval(tokens))\n",
    "# df.tags_pred = df.tags_pred.apply(lambda tags: ast.literal_eval(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.tokenized\n",
    "file_names = df.file_name\n",
    "categories = df.subCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_frequency = FreqDist()\n",
    "# for tokens in labels:\n",
    "#     tokens_frequency += FreqDist(tokens)\n",
    "\n",
    "# word_list = [token for token, freq in tokens_frequency.most_common()]\n",
    "\n",
    "# word_list = word_list[2:]\n",
    "\n",
    "# df_word_list = pd.DataFrame({\"word_list\": sorted(word_list)})\n",
    "\n",
    "# df_word_list.head()\n",
    "\n",
    "# df_word_list.to_csv(\"data/word_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_list = pd.read_csv(\"data/word_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a-line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa-chhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaliya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aandhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_list\n",
       "0    a-line\n",
       "1  aaa-chhe\n",
       "2    aaliya\n",
       "3    aandhi\n",
       "4     aaren"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, labels_train, labels_val = train_test_split(file_names, labels, test_size=0.3, random_state=105, stratify=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31010"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20980.jpg', '47190.jpg', '12943.jpg', ..., '10392.jpg',\n",
       "       '10569.jpg', '46051.jpg'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_train = X_train\n",
    "file_names_val = X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = list(labels_train)\n",
    "labels_val = list(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_names\n",
    "del labels\n",
    "del categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_list):\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "    vocab.add_word(start_token)\n",
    "    vocab.add_word(end_token)\n",
    "    vocab.add_word(unknown_token)\n",
    "    vocab.add_word(padding_token)\n",
    "    \n",
    "    for word in word_list:\n",
    "        vocab.add_word(word)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df_word_list.word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Caption Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor1d(tensor, split):\n",
    "    r\"\"\"Split 1D of tensor into N split\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "    Return\n",
    "        array of splitted tensor with N elements of array\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tensor[:split],\n",
    "        tensor[split: split * 2],\n",
    "        tensor[split * 2: split * 3],\n",
    "        tensor[split * 3:],\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_tensor2d(tensor, split, front=False):\n",
    "    r\"\"\"Split 2D of tensor into N split of 2D tensor\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "        front  (bool) : split axis 0 if True else axis 1\n",
    "    Return\n",
    "        array of splitted 2D tensor with N elements of array\n",
    "    \"\"\"\n",
    "\n",
    "    if front:\n",
    "        return [\n",
    "            tensor[:split, :],\n",
    "            tensor[split: split * 2, :],\n",
    "            tensor[split * 2: split * 3, :],\n",
    "            tensor[split * 3:, :],\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        tensor[:, :split],\n",
    "        tensor[:, split: split * 2],\n",
    "        tensor[:, split * 2: split * 3],\n",
    "        tensor[:, split * 3:],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexto1hot(vocab_len, index):\n",
    "    #print(\"index type: \")\n",
    "    if isinstance(index, int) == False:\n",
    "        n = len(index)\n",
    "        #print(\"making a 1hot encoding of shape: \" + str(n) + \",\" + str(vocab_len) )\n",
    "        one_hot = np.zeros([n,vocab_len])\n",
    "        #can this be optimized?\n",
    "        for i in range(n):\n",
    "            one_hot[i,index[i]]=1\n",
    "        \n",
    "        return one_hot\n",
    "    else:\n",
    "        one_hot = np.zeros([vocab_len])\n",
    "        one_hot[index] = 1\n",
    "        \n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, dim_word_emb, dim_lang_lstm, dim_image_feats, nb_hidden):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(dim_lang_lstm+dim_image_feats+dim_word_emb,\n",
    "                                     nb_hidden,\n",
    "                                     bias=True)\n",
    "        \n",
    "    def forward(self, h1, c1, h2, v_mean, word_emb):\n",
    "        #print(h2.shape)\n",
    "        #print(v_mean.shape)\n",
    "        #print(word_emb.shape)\n",
    "        input_feats = torch.cat((h2, v_mean, word_emb),dim=1)\n",
    "        h_out, c_out = self.lstm_cell(input_feats, (h1, c1))\n",
    "        \n",
    "        return h_out, c_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    def __init__(self, dim_image_feats, dim_att_lstm, nb_hidden):\n",
    "        super(Attend, self).__init__()\n",
    "    \n",
    "        self.fc_image_feats = nn.Linear(dim_image_feats, nb_hidden, bias=False)\n",
    "        self.fc_att_lstm = nn.Linear(dim_att_lstm, nb_hidden, bias=False)\n",
    "        self.act_tan = nn.Tanh()\n",
    "        self.fc_att = nn.Linear(nb_hidden, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, image_feats, h1):\n",
    "        nb_batch, nb_feats, feat_dim = image_feats.size()\n",
    "        att_lstm_emb = self.fc_att_lstm(h1).unsqueeze(1)\n",
    "        image_feats_emb = self.fc_image_feats(image_feats)\n",
    "        all_feats_emb = image_feats_emb + att_lstm_emb.repeat(1,nb_feats,1)\n",
    "\n",
    "        activate_feats = self.act_tan(all_feats_emb)\n",
    "        unnorm_attention = self.fc_att(activate_feats)\n",
    "        normed_attention = self.softmax(unnorm_attention)\n",
    "\n",
    "        #print(normed_attention.shape)\n",
    "        #print(nb_feats)\n",
    "        #print(image_feats.shape)\n",
    "        #weighted_feats = normed_attention.repeat(1,1,nb_feats) * image_feats\n",
    "        weighted_feats = normed_attention * image_feats\n",
    "        #print(weighted_feats.shape)\n",
    "        attended_image_feats = weighted_feats.sum(dim=1)\n",
    "        #print(attended_image_feats.shape)\n",
    "    \n",
    "        return attended_image_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageLSTM(nn.Module):\n",
    "    def __init__(self, dim_att_lstm, dim_visual_att, nb_hidden):\n",
    "        super(LanguageLSTM,self).__init__()\n",
    "        self.lstm_cell = nn.LSTMCell(dim_att_lstm+dim_visual_att,\n",
    "                                     nb_hidden,\n",
    "                                     bias=True)\n",
    "        \n",
    "    def forward(self, h2, c2, h1, v_hat):\n",
    "        #print(h1.shape)\n",
    "        #print(v_hat.shape)\n",
    "        input_feats = torch.cat((h1, v_hat),dim=1)\n",
    "        h_out, c_out = self.lstm_cell(input_feats, (h2, c2))\n",
    "        return h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence is class for beam search\n",
    "class Sentence(object):\n",
    "    def __init__(self, max_nb_words, beam_width, end_word, vocab):\n",
    "        self.max_nb_words = max_nb_words\n",
    "        self.beam_width = beam_width\n",
    "        self.end_word = end_word\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.words = []\n",
    "        self.probability = 0\n",
    "        self.ended = False\n",
    "\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "\n",
    "    def update_words(self, s, state, y):\n",
    "        y = self.act(y)\n",
    "        new_s = []\n",
    "        for i in range(self.beam_width):\n",
    "            val, idx = y.max(dim=1)\n",
    "            y[0, idx] -= val\n",
    "            current_word = y.clone()\n",
    "            current_word[0,:] = 0\n",
    "            current_word[0,idx] = 1\n",
    "            s2 = s.copy()\n",
    "            s2.update_state(val, state, current_word)\n",
    "            new_s.append(s2)\n",
    "            if s2.ended:\n",
    "                break\n",
    "        return new_s\n",
    "\n",
    "    def update_state(self, p, state, current_word):\n",
    "        self.state = [s.clone() for s in state]\n",
    "        self.words.append(current_word)\n",
    "        self._update_probability(p)\n",
    "        self._update_finished()\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.state, self.words[-1]\n",
    "\n",
    "    def extract_sentence(self):\n",
    "        sentence = []\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            sentence.append(self.vocab.get_word(idx))\n",
    "        return [self.probability, sentence]\n",
    "\n",
    "    def _update_probability(self, p):\n",
    "        self.probability += log(p, 2)\n",
    "\n",
    "    def _update_finished(self):\n",
    "        n = len(self.words)\n",
    "        f = self.words[-1]\n",
    "        if (n > self.max_nb_words) or (f == self.end_word).all():\n",
    "            self.ended = True\n",
    "\n",
    "    def copy(self):\n",
    "        new = Sentence(self.max_nb_words,\n",
    "                       self.beam_width,\n",
    "                       self.end_word,\n",
    "                       self.vocab)\n",
    "        new.words = [w.clone() for w in self.words]\n",
    "        new.probability = self.probability\n",
    "        return new\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.probability < other.probability\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            s += \"{}, \".format(self.vocab.get_word(idx))\n",
    "        return s\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, beam_width):\n",
    "        self.beam_width = beam_width\n",
    "        self.heap = []\n",
    "\n",
    "    def push(self, s):\n",
    "        s.probability *= -1\n",
    "        heapq.heappush(self.heap, s)\n",
    "\n",
    "    def pop(self):\n",
    "        s = heapq.heappop(self.heap)\n",
    "        s.probability *= -1\n",
    "        return s\n",
    "\n",
    "    def trim(self):\n",
    "        h2 = []\n",
    "        for i in range(self.beam_width):\n",
    "            if len(self.heap) == 0:\n",
    "                break\n",
    "            heapq.heappush(h2, heapq.heappop(self.heap))\n",
    "        self.heap=h2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictWord(nn.Module):\n",
    "    def __init__(self, dim_language_lstm, dict_size):\n",
    "        super(PredictWord, self).__init__()\n",
    "        self.fc = nn.Linear(dim_language_lstm, dict_size)\n",
    "        \n",
    "    def forward(self, h2):\n",
    "        y = self.fc(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zeros(shape, cuda=False):\n",
    "    zeros = torch.zeros(shape)\n",
    "    if cuda:\n",
    "        zeros = zeros.cuda()\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullProductDataset(Dataset):\n",
    "    \"\"\"Dataset for product with it's image, title token, and semantic input.\"\"\"\n",
    "\n",
    "    def __init__(self, file_names, titles, image_dir, vocab, transformer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.file_names = file_names\n",
    "        self.titles = titles\n",
    "        self.image_dir = image_dir\n",
    "        self.transformer = transformer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.file_names[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        \n",
    "        title = [vocab(token) for token in self.titles[idx]]\n",
    "        title = torch.tensor(title)\n",
    "        \n",
    "        print(img_name)\n",
    "     \n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(sample):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - feature: torch tensor of shape (36,2048).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        features: torch tensor of shape (batch_size, 36, 2048).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort data list by caption length (descending order).\n",
    "    sample.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, titles = zip(*sample)\n",
    "    \n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(title) for title in titles]\n",
    "\n",
    "    targets = torch.zeros(len(titles), 14).long()\n",
    "    for i, title in enumerate(titles):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = title[:end]\n",
    "\n",
    "    return torch.stack(images), targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images/20980.jpg\n",
      "0 (60, 80) torch.Size([7])\n",
      "data/images/47190.jpg\n",
      "1 (60, 80) torch.Size([7])\n",
      "data/images/12943.jpg\n",
      "2 (60, 80) torch.Size([9])\n",
      "data/images/51960.jpg\n",
      "3 (60, 80) torch.Size([7])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test dataset\n",
    "dataset = FullProductDataset(file_names_train, labels_train, \"data/images\", vocab)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, title = dataset[i]\n",
    "\n",
    "    print(i, image.size, title.shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformer = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoder Model from Image Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTagger(nn.Module):\n",
    "    r\"\"\"Tagger Encoder extends ResNet152 Model.\n",
    "\n",
    "    Arguments:\n",
    "        semantic_size (int, optional): size of semantic size\n",
    "        dropout (float, optional): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, semantic_size=1000, dropout=0.15):\n",
    "        super(ImageTagger, self).__init__()\n",
    "        self.semantic_size = semantic_size\n",
    "\n",
    "        # Using pre-trained ImageNet\n",
    "        resnet = torchvision.models.resnet152(pretrained=True)\n",
    "\n",
    "        # Remove linear layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(2048, semantic_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        r\"\"\"Forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            images (torch.Tensor): images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        Returns \n",
    "            torch.Tensor: probabilites of tags (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.view(out.size(0), -1)   # (batch_size, 2048)\n",
    "        out = self.dropout(out)           # (batch_size, 2048)\n",
    "        out = self.linear(out)            # (batch_size, 1000)\n",
    "        out = self.sigmoid(out)           # (batch_size, 1000)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        r\"\"\"Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "        Arguments\n",
    "            fine_tune (boolean): Allow fine tuning?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tagger = ImageTagger()\n",
    "image_tagger = image_tagger.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tagger.load_state_dict(torch.load(\"models/image_tagger.best_1\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = image_tagger.resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dict_size, image_feature_dim, vocab, tf_ratio):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.dict_size = dict_size\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.vocab = vocab\n",
    "        self.tf_ratio = tf_ratio\n",
    "\n",
    "        self.embed_word = nn.Linear(dict_size, WORD_EMB_DIM, bias=False)\n",
    "        self.lstm1 = AttentionLSTM(WORD_EMB_DIM,\n",
    "                                   NB_HIDDEN_LSTM2,\n",
    "                                   image_feature_dim,\n",
    "                                   NB_HIDDEN_LSTM1)\n",
    "        self.lstm2 = LanguageLSTM(NB_HIDDEN_LSTM1,\n",
    "                                  image_feature_dim,\n",
    "                                  NB_HIDDEN_LSTM2)\n",
    "        self.attention = Attend(image_feature_dim,\n",
    "                                NB_HIDDEN_LSTM1,\n",
    "                                NB_HIDDEN_ATT)\n",
    "        self.predict_word = PredictWord(NB_HIDDEN_LSTM2, dict_size)\n",
    "\n",
    "        self.h1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.c1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.h2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "        self.c2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "    \n",
    "    def forward(self, image_feats, nb_timesteps, true_words, beam=None):\n",
    "        if beam is not None:\n",
    "            return self.beam_search(image_feats, nb_timesteps, beam)\n",
    "\n",
    "        nb_batch, nb_image_feats, _ = image_feats.size()\n",
    "        use_cuda = image_feats.is_cuda\n",
    "\n",
    "        v_mean = image_feats.mean(dim=1)\n",
    "\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "        y_out = make_zeros((nb_batch, nb_timesteps-1, self.dict_size), cuda = use_cuda)\n",
    "\n",
    "        for t in range(nb_timesteps-1):\n",
    "            y, state = self.forward_one_step(state,\n",
    "                                             current_word,\n",
    "                                             v_mean,\n",
    "                                             image_feats)\n",
    "            y_out[:,t,:] = y\n",
    "\n",
    "            current_word = self.update_current_word(y, true_words, t, use_cuda)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "    def forward_one_step(self, state, current_word, v_mean, image_feats):\n",
    "        h1, c1, h2, c2 = state\n",
    "        word_emb = self.embed_word(current_word)\n",
    "        h1, c1 = self.lstm1(h1, c1, h2, v_mean, word_emb)\n",
    "        v_hat = self.attention(image_feats, h1)\n",
    "        h2, c2 = self.lstm2(h2, c2, h1, v_hat)\n",
    "        y = self.predict_word(h2)\n",
    "        state = [h1, c1, h2, c2]\n",
    "        \n",
    "        return y, state\n",
    "\n",
    "    def update_current_word(self, y, true_words, t, cuda):\n",
    "        use_tf = True if random.random() < self.tf_ratio else False\n",
    "        if use_tf:\n",
    "            next_word = true_words[:,t+1]\n",
    "        else:\n",
    "            next_word = torch.argmax(y, dim=1)\n",
    "\n",
    "        current_word = indexto1hot(len(self.vocab), next_word)\n",
    "        current_word = torch.from_numpy(current_word).float()\n",
    "        \n",
    "        if cuda:\n",
    "            current_word = current_word.cuda()\n",
    "    \n",
    "        return current_word\n",
    "\n",
    "    def init_inference(self, nb_batch, cuda):\n",
    "        start_word = indexto1hot(len(self.vocab), self.vocab('<start>'))\n",
    "        start_word = torch.from_numpy(start_word).float().unsqueeze(0)\n",
    "        start_word = start_word.repeat(nb_batch, 1)\n",
    "\n",
    "        if cuda:\n",
    "            start_word = start_word.cuda()\n",
    "\n",
    "        h1 = self.h1.repeat(nb_batch, 1)\n",
    "        c1 = self.c1.repeat(nb_batch, 1)\n",
    "        h2 = self.h2.repeat(nb_batch, 1)\n",
    "        c2 = self.c2.repeat(nb_batch, 1)\n",
    "        state = [h1, c1, h2, c2]\n",
    "\n",
    "        return state, start_word\n",
    "\n",
    "    #########################################\n",
    "    #               BEAM SEARCH             #\n",
    "    #########################################\n",
    "    def beam_search(self, image_features, max_nb_words, beam_width):\n",
    "        # Initialize model\n",
    "        use_cuda = image_features.is_cuda\n",
    "        nb_batch, nb_image_feats, _ = image_features.size()\n",
    "\n",
    "        v_mean = image_features.mean(dim=1)\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "\n",
    "        # Initialize beam search\n",
    "        end_word = indexto1hot(len(self.vocab), self.vocab('<end>'))\n",
    "        end_word = torch.from_numpy(end_word).float().unsqueeze(0)\n",
    "        end_word = end_word.cuda() if image_features.is_cuda else end_word\n",
    "        beam = Beam(beam_width)\n",
    "        s = Sentence(max_nb_words, beam_width, end_word, self.vocab)\n",
    "        s.update_state(1.0, state, current_word)\n",
    "        beam.push(s)\n",
    "\n",
    "        # Perform beam search\n",
    "        final_beam = Beam(beam_width)\n",
    "        while len(beam) > 0:\n",
    "            s = beam.pop()\n",
    "            new_sentences = self.update_states(s, image_features, v_mean)\n",
    "            for s in new_sentences:\n",
    "                if s.ended:\n",
    "                    final_beam.push(s)\n",
    "                else:\n",
    "                    beam.push(s)\n",
    "            # Get rid of low scoring sentences\n",
    "            beam.trim() \n",
    "            final_beam.trim()\n",
    "\n",
    "        # Extract final sentence\n",
    "        s = final_beam.pop() # Best sentence on top of heap\n",
    "        sentence = s.extract_sentence()\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def update_states(self, s, image_feats, v_mean):\n",
    "        state, current_word = s.get_states()\n",
    "        y, state = self.forward_one_step(state, current_word, v_mean, image_feats)\n",
    "        y = self.remove_consecutive_words(y, current_word)\n",
    "        new_sentences = s.update_words(s, state, y)\n",
    "        return new_sentences\n",
    "\n",
    "    def remove_consecutive_words(self, y, prev_word):\n",
    "        # give previous word low score so as not to repeat words\n",
    "        y = y - 10**10 * prev_word\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FullProductDataset(file_names_train, labels_train, \"data/images\", vocab, image_transformer)\n",
    "val_dataset = FullProductDataset(file_names_val, labels_val, \"data/images\", vocab, image_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(reference, hypothesis, ngram):\n",
    "    weights = [0, 0, 0, 0]\n",
    "    weights[0:ngram] = [1/ngram for i in range(ngram)]\n",
    "    \n",
    "    return sentence_bleu([reference], hypothesis, weights=tuple(weights), auto_reweigh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, encoder, train_loader, optimizer, kbar):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # forward\n",
    "        image_feats = encoder(images)\n",
    "        image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "        outputs = model(image_feats, len(captions[0]), captions)\n",
    "\n",
    "        captions = captions[:, 1:]\n",
    "\n",
    "        decode_lengths = [x-1 for x in lengths]\n",
    "        captions, _, _, _ = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
    "        outputs, _, _, _ = pack_padded_sequence(outputs, decode_lengths, batch_first = True)\n",
    "\n",
    "        batch_loss = loss(outputs, captions)\n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        kbar.update(i, values=[(\"loss\", batch_loss)])\n",
    "        \n",
    "    epoch_loss = epoch_loss / nb_batch \n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model, val_loader, vocab, beam=None):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    rouge_scores = []\n",
    "    bleu1_scores = []\n",
    "    bleu2_scores = []\n",
    "    bleu3_scores = []\n",
    "    bleu4_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            image_feats = encoder(images)\n",
    "            image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "            \n",
    "            outputs = model(image_feats, len(captions[0]), captions)\n",
    "\n",
    "            for j, (output, caption) in enumerate(zip(outputs, captions)):\n",
    "                caption_list = [vocab.get_word(wid.item()) for wid in caption if wid != 0]                \n",
    "                caption_list = caption_list[:-1]\n",
    "                caption_sentence = ' '.join(caption_list)\n",
    "\n",
    "                padded_result = [vocab.get_word(torch.argmax(one_hot_en).item()) for one_hot_en in output]\n",
    "                result_list = []\n",
    "                for word in padded_result:\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    result_list.append(word)\n",
    "                output_sentence = ' '.join(result_list)\n",
    "                    \n",
    "                rouge_scores.append(rouge_score(caption_sentence, output_sentence))\n",
    "                bleu1_scores.append(bleu_score(caption_list, result_list, 1))\n",
    "                bleu2_scores.append(bleu_score(caption_list, result_list, 2))\n",
    "                bleu3_scores.append(bleu_score(caption_list, result_list, 3))\n",
    "                try:\n",
    "                    bleu4_scores.append(bleu_score(caption_list, result_list, 4))\n",
    "                except:\n",
    "                    bleu4_scores.append(0)\n",
    "                    \n",
    "                print('ref', caption_sentence)\n",
    "                print('hyp', output_sentence)\n",
    "            \n",
    "            decode_lengths = [x-1 for x in lengths]\n",
    "            captions, _, _, _ = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
    "            outputs, _, _, _ = pack_padded_sequence(outputs, decode_lengths, batch_first = True)\n",
    "\n",
    "            batch_loss = loss(outputs, captions)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            \n",
    "            print('berak')\n",
    "            break\n",
    "    \n",
    "    rouge_score_avg = sum(rouge_scores)/len(rouge_scores)        \n",
    "    bleu1_score_avg = sum(bleu1_scores)/len(bleu1_scores)\n",
    "    bleu2_score_avg = sum(bleu2_scores)/len(bleu2_scores)\n",
    "    bleu3_score_avg = sum(bleu3_scores)/len(bleu3_scores)\n",
    "    bleu4_score_avg = sum(bleu4_scores)/len(bleu4_scores)\n",
    "    \n",
    "    print(rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg)\n",
    "    \n",
    "    epoch_loss = epoch_loss / nb_batch \n",
    "\n",
    "    return bleu3_score_avg, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab, optimizer, scheduler, max_epochs, current_epoch=0):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loss_array = []\n",
    "    val_loss_array = []\n",
    "    val_bleu3_array = []\n",
    "\n",
    "    # some big number\n",
    "    min_val_loss = 10**5\n",
    "    max_bleu3_score = 0\n",
    "    train_epoch_array = []\n",
    "    val_epoch_array = []\n",
    "    \n",
    "#     if current_epoch == 0:\n",
    "#         bleu3_score, val_loss = val_one_epoch(model, val_loader, vocab, beam=None)\n",
    "#         print(\"Validation loss with random initialization. Loss: \" + str(val_loss) + \", BLEU3 score: \" + str(bleu3_score))\n",
    "    \n",
    "    while current_epoch < max_epochs:\n",
    "        current_epoch += 1\n",
    "        \n",
    "        print('Epoch {}/{}'.format(current_epoch, max_epochs))\n",
    "        kbar = pkbar.Kbar(target=len(train_loader), width=32)\n",
    "        \n",
    "        train_loss = train_one_epoch(model, encoder, train_loader, optimizer, kbar)\n",
    "        bleu3_score, val_loss = val_one_epoch(model, val_loader, vocab, beam=None)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, \"models/pure_attention.epoch_\" + str(current_epoch))\n",
    "\n",
    "        kbar.add(1, values=[(\"loss\", train_loss), (\"val_loss\", val_loss), (\"BLEU3_score\", bleu3_score)])\n",
    "        \n",
    "        train_loss_array.append(train_loss)\n",
    "        val_loss_array.append(val_loss)\n",
    "        val_bleu3_array.append(bleu3_score)\n",
    "        train_epoch_array.append(current_epoch)\n",
    "        val_epoch_array.append(current_epoch)\n",
    "        \n",
    "        # keep track of the best model and save it\n",
    "        if bleu3_score > max_bleu3_score:\n",
    "            max_bleu3_score = bleu3_score\n",
    "            torch.save({\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, \"models/pure_attention.best_bleu3\")\n",
    "            \n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss            \n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\n')\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(min_val_loss))\n",
    "    print('Best val BLEU3 score: {:4f}'.format(max_bleu3_score))\n",
    "    \n",
    "    return model, train_loss_array, val_loss_array, val_bleu3_array, train_epoch_array, val_epoch_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMB_DIM = 1000\n",
    "NB_HIDDEN_LSTM1 = 1000\n",
    "NB_HIDDEN_LSTM2 = 1000\n",
    "NB_HIDDEN_ATT = 512\n",
    "\n",
    "IMAGE_FEATURE_DIM = 2048\n",
    "\n",
    "TF_RATIO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = CaptionModel(len(vocab), IMAGE_FEATURE_DIM, vocab, TF_RATIO)\n",
    "caption_model = caption_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "optimizer = optim.Adam(caption_model.parameters(), lr=lr)\n",
    "epochs = 10\n",
    "# optimizer = optim.SGD(model.parameters(),\n",
    "#                       lr=args.lr,\n",
    "#                       momentum = 0.899999976158,\n",
    "#                       weight_decay=0.000500000023749)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "checkpoint = torch.load(\"models/pure_attention.best\")\n",
    "caption_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "current_epoch = checkpoint['epoch']\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "ref converse men all star canvas ox navy blue shoe\n",
      "hyp converse men star star navy navy navy blue shoe\n",
      "ref hanes men pack of two premium bikini briefs\n",
      "hyp hanes men pack of two premium bikini briefs\n",
      "ref united colors of benetton men check blue shirts\n",
      "hyp united colors of benetton men check blue shirts\n",
      "ref doodle girl tonight night rock yellow pink kidswear\n",
      "hyp doodle girl saturday princess yellow yellow kidswear kidswear\n",
      "ref scullers for her women wow knit grey top\n",
      "hyp scullers for her women knit knit grey top\n",
      "ref batman men logo neon glow black t-shirt\n",
      "hyp batman men crew black black black t-shirt\n",
      "ref fossil men black dial chronograph watch ch\n",
      "hyp fossil men black dial chronograph watch ch\n",
      "ref femella women blue polka bow dress dress\n",
      "hyp femella women blue dress dress dress\n",
      "ref puma men alonso track red ferrari jacket\n",
      "hyp puma men red red red jacket jacket\n",
      "ref nike men transform iii black sports shoes\n",
      "hyp nike men transform black black sports shoes\n",
      "ref free authority men music machine black t-shirt\n",
      "hyp flying machine men music rider black t-shirt\n",
      "ref adidas women essential black candy pants\n",
      "hyp adidas women black black track track\n",
      "ref puma unisex fundamentals red water bottle\n",
      "hyp puma men red india red cap\n",
      "ref nike fragrances men pack of perfumes\n",
      "hyp nike fragrances women pack of perfumes of deos\n",
      "ref flying machine men solid black sweaters\n",
      "hyp flying machine men solid black sweater\n",
      "ref classic polo men striped purple t-shirt\n",
      "hyp classic polo men purple purple t-shirt\n",
      "ref puma women fitness quarters purple socks\n",
      "hyp puma women purple purple purple socks\n",
      "ref myntra men black banana diet t-shirt\n",
      "hyp myntra men black t-shirt t-shirt t-shirt\n",
      "ref lino perros women flower black handbag\n",
      "hyp lino perros women flower black handbag\n",
      "ref revlon extra fast nail polish remover\n",
      "hyp revlon sand mist mist polish\n",
      "ref mark taylor men plain blue shirts\n",
      "hyp mark taylor men blue blue shirts\n",
      "ref mumbai slang women printed purple top\n",
      "hyp mumbai slang women printed purple top\n",
      "ref myntra men blue banana diet t-shirt\n",
      "hyp myntra men blue t-shirt t-shirt t-shirt\n",
      "ref probase men white grafitti black tshirts\n",
      "hyp probase men white white white tshirts\n",
      "ref highlander men yellow black checked shirt\n",
      "hyp highlander men black black checked shirt\n",
      "ref jockey lcescbra men pack of socks\n",
      "hyp jockey lcescbra men pack of socks\n",
      "ref skechers men sports blue yellow shoe\n",
      "hyp skechers men gel blue shoe shoe\n",
      "ref mark taylor men green striped shirt\n",
      "hyp mark taylor men green striped shirt\n",
      "ref jealous women casual top black tops\n",
      "hyp jealous women black black black tops\n",
      "ref ucb women quarter sleeve pink top\n",
      "hyp ucb women short pink pink top\n",
      "ref belmonte men bright assorted steel cufflinks\n",
      "hyp belmonte men bright assorted steel cufflinks\n",
      "ref revlon photo ready fair light compact\n",
      "hyp revlon photo ready medium compact compact\n",
      "ref proline men charcoal grey polo t-shirt\n",
      "hyp proline men grey grey polo t-shirt\n",
      "ref locomotive men ribbed neck blue t-shirt\n",
      "hyp locomotive men blue blue navy t-shirt\n",
      "ref puma unisex lo green white shoes\n",
      "hyp puma unisex lo lo shoes shoes\n",
      "ref adidas men raggmo grey flip flops\n",
      "hyp adidas men grey grey flip flops\n",
      "ref revlon velvet touch royal burgundy lipstick\n",
      "hyp revlon velvet touch super lipstick lipstick\n",
      "ref wildcraft unisex contour red backpack\n",
      "hyp wildcraft unisex red red backpack\n",
      "ref fabindia women green crochet purse\n",
      "hyp fabindia women green purse purse\n",
      "ref lee men checks blue shirt\n",
      "hyp lee men blue mason shirt\n",
      "ref basics men solid purple tshirts\n",
      "hyp basics men purple purple t-shirt\n",
      "ref nike unisex allegian blue backpack\n",
      "hyp nike unisex blue blue backpack\n",
      "ref lee men hand green t-shirt\n",
      "hyp lee men mirror wassup t-shirt\n",
      "ref helix women golden dial watch\n",
      "hyp fastrack women white dial watch\n",
      "ref happy socks unisex black socks\n",
      "hyp happy socks unisex black socks\n",
      "ref locomotive men euro blue jeans\n",
      "hyp locomotive men blue blue jeans\n",
      "ref levi men printed white tshirts\n",
      "hyp levi men printed white tshirts\n",
      "ref colorbar soft touch felin lipstick\n",
      "hyp colorbar soft touch show show\n",
      "ref mr men printed yellow t-shirt\n",
      "hyp mark men printed yellow printed\n",
      "ref aurelia women solid purple kurtas\n",
      "hyp women women solid purple kurtas\n",
      "ref aneri women madhuri yellow kurta\n",
      "hyp aneri women sagar yellow kurta\n",
      "ref lee men check blue shirts\n",
      "hyp lee men check blue shirts\n",
      "ref york men blue berry perfume\n",
      "hyp york men blue perfume perfume\n",
      "ref murcia women brown leather handbag\n",
      "hyp murcia women brown leather brown\n",
      "ref van heusen unisex sunglasses vh-c\n",
      "hyp van heusen unisex brown vh-c\n",
      "ref proline blue patterned muffler\n",
      "hyp proline blue striped muffler\n",
      "ref rreverie sea green earrings\n",
      "hyp rreverie green green earrings\n",
      "ref puma men blue t-shirt\n",
      "hyp puma men blue t-shirt\n",
      "ref hakashi men black cufflinks\n",
      "hyp hakashi men black cufflinks\n",
      "ref hm women black heels\n",
      "hyp hm women black heels\n",
      "ref portia women silver flats\n",
      "hyp portia women white flats\n",
      "ref hakashi men black cufflinks\n",
      "hyp hakashi men black cufflinks\n",
      "ref estelle women gold bracelet\n",
      "hyp estelle women gold bracelet\n",
      "ref remanika women black tunic\n",
      "hyp remanika women black dress\n",
      "berak\n",
      "0.7797427832584083 0.7746955122852982 0.6499536222986717 0.44825657023723864 0.2905830671188193\n",
      "  1/970 [................................] - ETA: 40:55 - loss: 0.0000e+00 - val_loss: 0.0103 - BLEU3_score: 0.4483\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.010255\n",
      "Best val BLEU3 score: 0.448257\n"
     ]
    }
   ],
   "source": [
    "caption_model, train_loss_array, val_loss_array, val_bleu3_array, train_epoch_array, val_epoch_array = train(\n",
    "    caption_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5160518164055924"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(val_bleu3_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_epoch_array, val_bleu3_array, label = 'Validation BLEU3 score')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BLEU3 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(reference, hypothesis):\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-l'])\n",
    "    return evaluator.get_scores([hypothesis], [reference])['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_loader, vocab, beam=5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    rouge_scores = []\n",
    "    bleu1_scores = []\n",
    "    bleu2_scores = []\n",
    "    bleu3_scores = []\n",
    "    bleu4_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            image_feats = encoder(images)\n",
    "            image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "            \n",
    "            outputs = model(image_feats, len(captions[0]), captions)\n",
    "\n",
    "            for j, (output, caption) in enumerate(zip(outputs, captions)):\n",
    "                caption_list = [vocab.get_word(wid.item()) for wid in caption if wid != 0]                \n",
    "                caption_list = caption_list[:-1]\n",
    "                caption_sentence = ' '.join(caption_list)\n",
    "\n",
    "                padded_result = [vocab.get_word(torch.argmax(one_hot_en).item()) for one_hot_en in output]\n",
    "                result_list = []\n",
    "                for word in padded_result:\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    result_list.append(word)\n",
    "                output_sentence = ' '.join(result_list)\n",
    "                    \n",
    "                rouge_scores.append(rouge_score(caption_sentence, output_sentence))\n",
    "                bleu1_scores.append(bleu_score(caption_list, result_list, 1))\n",
    "                bleu2_scores.append(bleu_score(caption_list, result_list, 2))\n",
    "                bleu3_scores.append(bleu_score(caption_list, result_list, 3))\n",
    "                try:\n",
    "                    bleu4_scores.append(bleu_score(caption_list, result_list, 4))\n",
    "                except:\n",
    "                    bleu4_scores.append(0)\n",
    "                    \n",
    "                print('ref', caption_sentence)\n",
    "                print('hyp', output_sentence)\n",
    "            \n",
    "            decode_lengths = [x-1 for x in lengths]\n",
    "            captions, _, _, _ = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
    "            outputs, _, _, _ = pack_padded_sequence(outputs, decode_lengths, batch_first = True)\n",
    "\n",
    "            batch_loss = loss(outputs, captions)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            \n",
    "            break\n",
    "    \n",
    "    rouge_score_avg = sum(rouge_scores)/len(rouge_scores)        \n",
    "    bleu1_score_avg = sum(bleu1_scores)/len(bleu1_scores)\n",
    "    bleu2_score_avg = sum(bleu2_scores)/len(bleu2_scores)\n",
    "    bleu3_score_avg = sum(bleu3_scores)/len(bleu3_scores)\n",
    "    bleu4_score_avg = sum(bleu4_scores)/len(bleu4_scores)\n",
    "    \n",
    "    return rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullProductDataset(Dataset):\n",
    "    \"\"\"Dataset for product with it's image, title token, and semantic input.\"\"\"\n",
    "\n",
    "    def __init__(self, file_names, titles, image_dir, vocab, transformer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.file_names = file_names\n",
    "        self.titles = titles\n",
    "        self.image_dir = image_dir\n",
    "        self.transformer = transformer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.file_names[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        \n",
    "        title = [vocab(token) for token in self.titles[idx]]\n",
    "        title = torch.tensor(title)\n",
    "        \n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(sample):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - feature: torch tensor of shape (36,2048).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        features: torch tensor of shape (batch_size, 36, 2048).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort data list by caption length (descending order).\n",
    "    sample.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, titles = zip(*sample)\n",
    "    \n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(title) for title in titles]\n",
    "\n",
    "    targets = torch.zeros(len(titles), max(lengths)).long()\n",
    "    for i, title in enumerate(titles):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = title[:end]\n",
    "\n",
    "    return torch.stack(images), targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,   71, 7064, 2150,  711, 1031, 4595,    1])\n",
      "data/images/2114.jpg\n",
      "tensor([   0, 5028, 6714, 2561, 5175, 6945,  801,    1])\n",
      "data/images/8958.jpg\n",
      "(tensor([   0,   71, 7064, 2150,  711, 1031, 4595,    1]), tensor([   0, 5028, 6714, 2561, 5175, 6945,  801,    1]))\n",
      "ref adidas women essential black candy pants\n",
      "hyp jealous women black black trousers jeans\n",
      "ref puma unisex fundamentals red water bottle\n",
      "hyp puma men white white blue t-shirt\n"
     ]
    }
   ],
   "source": [
    "val_dataset = FullProductDataset(file_names_val, labels_val, \"data/images\", vocab, image_transformer)\n",
    "pred_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg = predict(caption_model, pred_loader, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref mr men men mr noisy blue t-shirt\n",
      "hyp mr men men mr blue blue t-shirt\n",
      "ref playboy women playmate lavender ankle socks\n",
      "hyp playboy women purple purple ankle socks\n",
      "ref wildcraft unisex black and grey backpack\n",
      "hyp wildcraft unisex black and black backpack\n",
      "ref ice unisex sili pink watch\n",
      "hyp ice originals sili pink watch\n",
      "ref vishudh women printed purple kurta\n",
      "hyp women women printed brown kurta\n",
      "ref pitaraa golden fish scale necklace\n",
      "hyp toniq golden golden scale necklace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "id_ = [19766,\n",
    " 32863,\n",
    " 2845,\n",
    " 32272,\n",
    " 32157,\n",
    " 48332,]\n",
    "test_df = df[df.id.isin(id_)]\n",
    "file_names_train = test_df['file_name'].values\n",
    "semantic_inputs_train = test_df['tags_pred'].values\n",
    "labels_train = test_df['tokenized'].values\n",
    "\n",
    "test_dataset = FullProductDataset(file_names_train, labels_train, \"data/images\", vocab, image_transformer)\n",
    "pred_loader = DataLoader(test_dataset, batch_size=6, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg = predict(caption_model, pred_loader, vocab, beam=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5856410256410257 0.5457142857142857 0.4286288703974934 0.21988224694339595 0.1519671371303185\n"
     ]
    }
   ],
   "source": [
    "print(rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = [2114, 7822, 18412, 59501, 25076, 51465]\n",
    "test_df = df[df.id.isin(id_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2114</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>ADIDAS Women Essential Black Candy Pants</td>\n",
       "      <td>[&lt;start&gt;, adidas, women, essential, black, can...</td>\n",
       "      <td>2114.jpg</td>\n",
       "      <td>['adidas', 'women', 'essential', 'black', 'pan...</td>\n",
       "      <td>[6.266715e-07, 8.480771e-09, 1.4408267e-08, 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>7822</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Puma Women Corsica Mid L Shine Grey Shoe</td>\n",
       "      <td>[&lt;start&gt;, puma, women, corsica, mid, shine, gr...</td>\n",
       "      <td>7822.jpg</td>\n",
       "      <td>['puma', 'women', 'mid', 'shine', 'grey', 'shoe']</td>\n",
       "      <td>[0.00033552712, 7.675066e-08, 8.196119e-06, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>18412</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>White</td>\n",
       "      <td>Skechers MenShape-Ups White Shoe</td>\n",
       "      <td>[&lt;start&gt;, skechers, menshape-ups, white, shoe,...</td>\n",
       "      <td>18412.jpg</td>\n",
       "      <td>['skechers', 'white', 'shoe']</td>\n",
       "      <td>[1.3006752e-06, 3.6444412e-08, 3.2892455e-08, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19310</th>\n",
       "      <td>25076</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Accessory Gift Set</td>\n",
       "      <td>Black</td>\n",
       "      <td>Lino Perros Men Formal Black Accessory Gift Set</td>\n",
       "      <td>[&lt;start&gt;, lino, perros, men, formal, black, ac...</td>\n",
       "      <td>25076.jpg</td>\n",
       "      <td>['lino', 'perros', 'men', 'formal', 'black', '...</td>\n",
       "      <td>[9.770855e-07, 8.2946455e-11, 3.8270676e-10, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38207</th>\n",
       "      <td>51465</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Teal</td>\n",
       "      <td>Tonga Women Teal Dress</td>\n",
       "      <td>[&lt;start&gt;, tonga, women, teal, dress, &lt;end&gt;]</td>\n",
       "      <td>51465.jpg</td>\n",
       "      <td>['tonga', 'women', 'teal', 'dress']</td>\n",
       "      <td>[1.3901667e-08, 5.3713356e-10, 3.3170322e-14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44023</th>\n",
       "      <td>59501</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Jewellery</td>\n",
       "      <td>Pendant</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Lucera Gold Plated Pendant With Chain</td>\n",
       "      <td>[&lt;start&gt;, lucera, gold, plated, pendant, with,...</td>\n",
       "      <td>59501.jpg</td>\n",
       "      <td>['lucera', 'gold', 'plated', 'pendant', 'with'...</td>\n",
       "      <td>[2.7354895e-06, 7.0996786e-05, 8.538294e-07, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id masterCategory  subCategory         articleType baseColour  \\\n",
       "376     2114        Apparel   Bottomwear         Track Pants      Black   \n",
       "5334    7822       Footwear        Shoes        Casual Shoes       Grey   \n",
       "14127  18412       Footwear        Shoes        Sports Shoes      White   \n",
       "19310  25076    Accessories  Accessories  Accessory Gift Set      Black   \n",
       "38207  51465        Apparel        Dress             Dresses       Teal   \n",
       "44023  59501    Accessories    Jewellery             Pendant       Gold   \n",
       "\n",
       "                                                 title  \\\n",
       "376           ADIDAS Women Essential Black Candy Pants   \n",
       "5334          Puma Women Corsica Mid L Shine Grey Shoe   \n",
       "14127                 Skechers MenShape-Ups White Shoe   \n",
       "19310  Lino Perros Men Formal Black Accessory Gift Set   \n",
       "38207                           Tonga Women Teal Dress   \n",
       "44023            Lucera Gold Plated Pendant With Chain   \n",
       "\n",
       "                                               tokenized  file_name  \\\n",
       "376    [<start>, adidas, women, essential, black, can...   2114.jpg   \n",
       "5334   [<start>, puma, women, corsica, mid, shine, gr...   7822.jpg   \n",
       "14127  [<start>, skechers, menshape-ups, white, shoe,...  18412.jpg   \n",
       "19310  [<start>, lino, perros, men, formal, black, ac...  25076.jpg   \n",
       "38207        [<start>, tonga, women, teal, dress, <end>]  51465.jpg   \n",
       "44023  [<start>, lucera, gold, plated, pendant, with,...  59501.jpg   \n",
       "\n",
       "                                                    tags  \\\n",
       "376    ['adidas', 'women', 'essential', 'black', 'pan...   \n",
       "5334   ['puma', 'women', 'mid', 'shine', 'grey', 'shoe']   \n",
       "14127                      ['skechers', 'white', 'shoe']   \n",
       "19310  ['lino', 'perros', 'men', 'formal', 'black', '...   \n",
       "38207                ['tonga', 'women', 'teal', 'dress']   \n",
       "44023  ['lucera', 'gold', 'plated', 'pendant', 'with'...   \n",
       "\n",
       "                                               tags_pred  \n",
       "376    [6.266715e-07, 8.480771e-09, 1.4408267e-08, 7....  \n",
       "5334   [0.00033552712, 7.675066e-08, 8.196119e-06, 1....  \n",
       "14127  [1.3006752e-06, 3.6444412e-08, 3.2892455e-08, ...  \n",
       "19310  [9.770855e-07, 8.2946455e-11, 3.8270676e-10, 0...  \n",
       "38207  [1.3901667e-08, 5.3713356e-10, 3.3170322e-14, ...  \n",
       "44023  [2.7354895e-06, 7.0996786e-05, 8.538294e-07, 3...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
