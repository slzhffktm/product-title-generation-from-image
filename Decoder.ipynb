{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import ast\n",
    "import heapq\n",
    "from copy import deepcopy\n",
    "from math import log\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import pkbar\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skimage import io\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import FreqDist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "unknown_token = '<unk>'\n",
    "padding_token = '<pad>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/styles_tags_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1163</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Sahara Team India Fanwear Round Neck Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'sahara', 'team', 'india',...</td>\n",
       "      <td>1163.jpg</td>\n",
       "      <td>['nike', 'team', 'india', 'round', 'neck', 'je...</td>\n",
       "      <td>[2.3143535e-07, 2.3702476e-09, 2.986898e-07, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Men Blue T20 Indian Cricket Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'men', 'blue', 'indian', '...</td>\n",
       "      <td>1164.jpg</td>\n",
       "      <td>['nike', 'men', 'blue', 'indian', 'cricket', '...</td>\n",
       "      <td>[2.2555818e-07, 1.8278894e-09, 7.16435e-09, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1165</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Nike Mean Team India Cricket Jersey</td>\n",
       "      <td>['&lt;start&gt;', 'nike', 'mean', 'team', 'india', '...</td>\n",
       "      <td>1165.jpg</td>\n",
       "      <td>['nike', 'team', 'india', 'cricket', 'jersey']</td>\n",
       "      <td>[0.00035982736, 2.8710444e-06, 5.1504063e-05, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1525</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Puma Deck Navy Blue Backpack</td>\n",
       "      <td>['&lt;start&gt;', 'puma', 'deck', 'navy', 'blue', 'b...</td>\n",
       "      <td>1525.jpg</td>\n",
       "      <td>['puma', 'navy', 'blue', 'backpack']</td>\n",
       "      <td>[1.9060193e-05, 8.293974e-09, 9.6082715e-08, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1526</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>Black</td>\n",
       "      <td>Puma Big Cat Backpack Black</td>\n",
       "      <td>['&lt;start&gt;', 'puma', 'big', 'cat', 'backpack', ...</td>\n",
       "      <td>1526.jpg</td>\n",
       "      <td>['puma', 'big', 'cat', 'backpack', 'black']</td>\n",
       "      <td>[0.0015358106, 1.2226741e-07, 7.6661183e-07, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id masterCategory subCategory articleType baseColour  \\\n",
       "0  1163        Apparel     Topwear     Tshirts       Blue   \n",
       "1  1164        Apparel     Topwear     Tshirts       Blue   \n",
       "2  1165        Apparel     Topwear     Tshirts       Blue   \n",
       "3  1525    Accessories        Bags   Backpacks  Navy Blue   \n",
       "4  1526    Accessories        Bags   Backpacks      Black   \n",
       "\n",
       "                                              title  \\\n",
       "0  Nike Sahara Team India Fanwear Round Neck Jersey   \n",
       "1           Nike Men Blue T20 Indian Cricket Jersey   \n",
       "2               Nike Mean Team India Cricket Jersey   \n",
       "3                      Puma Deck Navy Blue Backpack   \n",
       "4                       Puma Big Cat Backpack Black   \n",
       "\n",
       "                                           tokenized file_name  \\\n",
       "0  ['<start>', 'nike', 'sahara', 'team', 'india',...  1163.jpg   \n",
       "1  ['<start>', 'nike', 'men', 'blue', 'indian', '...  1164.jpg   \n",
       "2  ['<start>', 'nike', 'mean', 'team', 'india', '...  1165.jpg   \n",
       "3  ['<start>', 'puma', 'deck', 'navy', 'blue', 'b...  1525.jpg   \n",
       "4  ['<start>', 'puma', 'big', 'cat', 'backpack', ...  1526.jpg   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['nike', 'team', 'india', 'round', 'neck', 'je...   \n",
       "1  ['nike', 'men', 'blue', 'indian', 'cricket', '...   \n",
       "2     ['nike', 'team', 'india', 'cricket', 'jersey']   \n",
       "3               ['puma', 'navy', 'blue', 'backpack']   \n",
       "4        ['puma', 'big', 'cat', 'backpack', 'black']   \n",
       "\n",
       "                                           tags_pred  \n",
       "0  [2.3143535e-07, 2.3702476e-09, 2.986898e-07, 3...  \n",
       "1  [2.2555818e-07, 1.8278894e-09, 7.16435e-09, 3....  \n",
       "2  [0.00035982736, 2.8710444e-06, 5.1504063e-05, ...  \n",
       "3  [1.9060193e-05, 8.293974e-09, 9.6082715e-08, 2...  \n",
       "4  [0.0015358106, 1.2226741e-07, 7.6661183e-07, 9...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masterCategory</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accessories</th>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apparel</th>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21389</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "      <td>21395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footwear</th>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "      <td>9220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Free Items</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Care</th>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2395</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sporting Goods</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  subCategory  articleType  baseColour  title  tokenized  \\\n",
       "masterCategory                                                                  \n",
       "Accessories     11286        11286        11286       11286  11286      11286   \n",
       "Apparel         21395        21395        21395       21389  21395      21395   \n",
       "Footwear         9220         9220         9220        9220   9220       9220   \n",
       "Free Items        105          105          105         105    105        105   \n",
       "Home                1            1            1           1      1          1   \n",
       "Personal Care    2399         2399         2399        2395   2399       2399   \n",
       "Sporting Goods     25           25           25          25     25         25   \n",
       "\n",
       "                file_name   tags  tags_pred  \n",
       "masterCategory                               \n",
       "Accessories         11286  11286      11286  \n",
       "Apparel             21395  21395      21395  \n",
       "Footwear             9220   9220       9220  \n",
       "Free Items            105    105        105  \n",
       "Home                    1      1          1  \n",
       "Personal Care        2399   2399       2399  \n",
       "Sporting Goods         25     25         25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"masterCategory\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_master_category = [\n",
    "    \"Home\",\n",
    "    \"Sporting Goods\",\n",
    "    \"Free Items\"\n",
    "]\n",
    "\n",
    "df = df[~df.masterCategory.isin(removed_master_category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stringified-list-type columns back to list\n",
    "df.tokenized = df.tokenized.apply(lambda tokens: ast.literal_eval(tokens))\n",
    "df.tags_pred = df.tags_pred.apply(lambda tags: ast.literal_eval(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.tokenized\n",
    "semantic_inputs = df.tags_pred\n",
    "file_names = df.file_name\n",
    "categories = df.subCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_frequency = FreqDist()\n",
    "# for tokens in labels:\n",
    "#     tokens_frequency += FreqDist(tokens)\n",
    "\n",
    "# word_list = [token for token, freq in tokens_frequency.most_common()]\n",
    "\n",
    "# word_list = word_list[2:]\n",
    "\n",
    "# df_word_list = pd.DataFrame({\"word_list\": sorted(word_list)})\n",
    "\n",
    "# df_word_list.head()\n",
    "\n",
    "# df_word_list.to_csv(\"data/word_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_list = pd.read_csv(\"data/word_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a-line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa-chhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaliya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aandhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_list\n",
       "0    a-line\n",
       "1  aaa-chhe\n",
       "2    aaliya\n",
       "3    aandhi\n",
       "4     aaren"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, labels_train, labels_val = train_test_split(list(zip(file_names, semantic_inputs)), labels, test_size=0.2, random_state=105, stratify=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_train = X_train[:, 0]\n",
    "semantic_inputs_train = X_train[:, 1]\n",
    "file_names_val = X_val[:, 0]\n",
    "semantic_inputs_val = X_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = list(labels_train)\n",
    "labels_val = list(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_names\n",
    "del labels\n",
    "del semantic_inputs\n",
    "del categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_list):\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "    vocab.add_word(start_token)\n",
    "    vocab.add_word(end_token)\n",
    "    vocab.add_word(unknown_token)\n",
    "    vocab.add_word(padding_token)\n",
    "    \n",
    "    for word in word_list:\n",
    "        vocab.add_word(word)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df_word_list.word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Caption Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor1d(tensor, split):\n",
    "    r\"\"\"Split 1D of tensor into N split\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "    Return\n",
    "        array of splitted tensor with N elements of array\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tensor[:split],\n",
    "        tensor[split: split * 2],\n",
    "        tensor[split * 2: split * 3],\n",
    "        tensor[split * 3:],\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_tensor2d(tensor, split, front=False):\n",
    "    r\"\"\"Split 2D of tensor into N split of 2D tensor\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "        front  (bool) : split axis 0 if True else axis 1\n",
    "    Return\n",
    "        array of splitted 2D tensor with N elements of array\n",
    "    \"\"\"\n",
    "\n",
    "    if front:\n",
    "        return [\n",
    "            tensor[:split, :],\n",
    "            tensor[split: split * 2, :],\n",
    "            tensor[split * 2: split * 3, :],\n",
    "            tensor[split * 3:, :],\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        tensor[:, :split],\n",
    "        tensor[:, split: split * 2],\n",
    "        tensor[:, split * 2: split * 3],\n",
    "        tensor[:, split * 3:],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexto1hot(vocab_len, index):\n",
    "    #print(\"index type: \")\n",
    "    if isinstance(index, int) == False:\n",
    "        n = len(index)\n",
    "        #print(\"making a 1hot encoding of shape: \" + str(n) + \",\" + str(vocab_len) )\n",
    "        one_hot = np.zeros([n,vocab_len])\n",
    "        #can this be optimized?\n",
    "        for i in range(n):\n",
    "            one_hot[i,index[i]]=1\n",
    "        \n",
    "        return one_hot\n",
    "    else:\n",
    "        one_hot = np.zeros([vocab_len])\n",
    "        one_hot[index] = 1\n",
    "        \n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, dim_word_emb, dim_lang_lstm, dim_image_feats, nb_hidden):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(dim_lang_lstm+dim_image_feats+dim_word_emb,\n",
    "                                     nb_hidden,\n",
    "                                     bias=True)\n",
    "        \n",
    "    def forward(self, h1, c1, h2, v_mean, word_emb):\n",
    "        #print(h2.shape)\n",
    "        #print(v_mean.shape)\n",
    "        #print(word_emb.shape)\n",
    "        input_feats = torch.cat((h2, v_mean, word_emb),dim=1)\n",
    "        h_out, c_out = self.lstm_cell(input_feats, (h1, c1))\n",
    "        \n",
    "        return h_out, c_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    def __init__(self, dim_image_feats, dim_att_lstm, nb_hidden):\n",
    "        super(Attend, self).__init__()\n",
    "    \n",
    "        self.fc_image_feats = nn.Linear(dim_image_feats, nb_hidden, bias=False)\n",
    "        self.fc_att_lstm = nn.Linear(dim_att_lstm, nb_hidden, bias=False)\n",
    "        self.act_tan = nn.Tanh()\n",
    "        self.fc_att = nn.Linear(nb_hidden, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, image_feats, h1):\n",
    "        nb_batch, nb_feats, feat_dim = image_feats.size()\n",
    "        att_lstm_emb = self.fc_att_lstm(h1).unsqueeze(1)\n",
    "        image_feats_emb = self.fc_image_feats(image_feats)\n",
    "        all_feats_emb = image_feats_emb + att_lstm_emb.repeat(1,nb_feats,1)\n",
    "\n",
    "        activate_feats = self.act_tan(all_feats_emb)\n",
    "        unnorm_attention = self.fc_att(activate_feats)\n",
    "        normed_attention = self.softmax(unnorm_attention)\n",
    "\n",
    "        #print(normed_attention.shape)\n",
    "        #print(nb_feats)\n",
    "        #print(image_feats.shape)\n",
    "        #weighted_feats = normed_attention.repeat(1,1,nb_feats) * image_feats\n",
    "        weighted_feats = normed_attention * image_feats\n",
    "        #print(weighted_feats.shape)\n",
    "        attended_image_feats = weighted_feats.sum(dim=1)\n",
    "        #print(attended_image_feats.shape)\n",
    "    \n",
    "        return attended_image_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNCell(nn.Module):\n",
    "    r\"\"\"Custom Cell for Implementing Semantic Compositional Networks\n",
    "\n",
    "    Arguments\n",
    "        input_size (int): size of input\n",
    "        hidden_size (int): size of embedding\n",
    "        semantic_size (int): size of semantic\n",
    "        factor_size (int): size of factor\n",
    "        bias (boolean, optional): use bias?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, semantic_size, factor_size, bias=True):\n",
    "        super(SCNCell, self).__init__()\n",
    "\n",
    "        self.factor_size = factor_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.semantic_size = semantic_size\n",
    "\n",
    "        self.weight_ia = nn.Parameter(\n",
    "            torch.Tensor(input_size, 4 * factor_size))\n",
    "        self.weight_ib = nn.Parameter(\n",
    "            torch.Tensor(semantic_size, 4 * factor_size))\n",
    "        self.weight_ic = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "\n",
    "        self.weight_ha = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "        self.weight_hb = nn.Parameter(\n",
    "            torch.Tensor(semantic_size, 4 * factor_size))\n",
    "        self.weight_hc = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "\n",
    "        if bias:\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, h1, v_hat, semantic_input, hx=None):\n",
    "        r\"\"\"Forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            wemb_input (torch.Tensor): word embedding input, a tensor of dimension (batch_size, input_size)\n",
    "            semantic_input (torch.Tensor): semantic concepts input, a tensor of dimension (batch_size, semantic_dim)\n",
    "        Returns\n",
    "            torch.Tensor: next hidden state, next cell state\n",
    "        \"\"\"\n",
    "\n",
    "        input_feats = torch.cat((h1, v_hat), dim=1)\n",
    "        \n",
    "        self.check_forward_input(input_feats)\n",
    "\n",
    "        [ia_i, ia_f, ia_o, ia_c] = split_tensor2d(\n",
    "            self.weight_ia, self.factor_size)\n",
    "        [ib_i, ib_f, ib_o, ib_c] = split_tensor2d(\n",
    "            self.weight_ib, self.factor_size)\n",
    "        [ic_i, ic_f, ic_o, ic_c] = split_tensor2d(\n",
    "            self.weight_ic, self.factor_size)\n",
    "        [b_ii, b_if, b_io, b_ic] = split_tensor1d(\n",
    "            self.bias_ih, self.hidden_size)\n",
    "\n",
    "        tmp1_i = (input_feats @ ia_i)\n",
    "        tmp1_f = (input_feats @ ia_f)\n",
    "        tmp1_o = (input_feats @ ia_o)\n",
    "        tmp1_c = (input_feats @ ia_c)\n",
    "\n",
    "        tmp2_i = (semantic_input @ ib_i).unsqueeze(0)\n",
    "        tmp2_f = (semantic_input @ ib_f).unsqueeze(0)\n",
    "        tmp2_o = (semantic_input @ ib_o).unsqueeze(0)\n",
    "        tmp2_c = (semantic_input @ ib_c).unsqueeze(0)\n",
    "\n",
    "        state_below_i = ((tmp1_i * tmp2_i) @ ic_i.t()) + b_ii\n",
    "        state_below_f = ((tmp1_f * tmp2_f) @ ic_f.t()) + b_if\n",
    "        state_below_o = ((tmp1_o * tmp2_o) @ ic_o.t()) + b_io\n",
    "        state_below_c = ((tmp1_c * tmp2_c) @ ic_c.t()) + b_ic\n",
    "\n",
    "        x_i = state_below_i.squeeze(0)\n",
    "        x_f = state_below_f.squeeze(0)\n",
    "        x_o = state_below_o.squeeze(0)\n",
    "        x_c = state_below_c.squeeze(0)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = input_feats.new_zeros(input_feats.size(\n",
    "                0), self.hidden_size, requires_grad=False)\n",
    "            hx = (hx, hx)\n",
    "\n",
    "        self.check_forward_hidden(x_i, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_i, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_f, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_f, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_o, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_o, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_c, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_c, hx[1], '[1]')\n",
    "\n",
    "        return self.recurrent_step(x_i, x_f, x_o, x_c, semantic_input, hx)\n",
    "\n",
    "    def recurrent_step(self, x_i, x_f, x_o, x_c, semantic_input, hx):\n",
    "        r\"\"\"Recurrent step helper for forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            x_i, x_f, x_o, x_c (torch.Tensor): factorized input, containing information from word embedding and semantic concepts, tensors of dimension (batch_size, input_size)\n",
    "            semantic_input (torch.Tensor): semantic concepts input, a tensor of dimension (batch_size, semantic_dim)\n",
    "            h_x (torch.Tensor): initial value of hidden and cell state\n",
    "        Returns\n",
    "            torch.Tensor: next hidden state, next cell state\n",
    "        \"\"\"\n",
    "\n",
    "        h_, c_ = hx\n",
    "\n",
    "        [ha_i, ha_f, ha_o, ha_c] = split_tensor2d(\n",
    "            self.weight_ha, self.factor_size)\n",
    "        [hb_i, hb_f, hb_o, hb_c] = split_tensor2d(\n",
    "            self.weight_hb, self.factor_size)\n",
    "        [hc_i, hc_f, hc_o, hc_c] = split_tensor2d(\n",
    "            self.weight_hc, self.factor_size)\n",
    "        [b_hi, b_hf, b_ho, b_hc] = split_tensor1d(\n",
    "            self.bias_hh, self.hidden_size)\n",
    "\n",
    "        preact_i = (h_ @ ha_i) * (semantic_input @ hb_i)\n",
    "        preact_i = (preact_i @ hc_i.t()) + x_i + b_hi\n",
    "\n",
    "        preact_f = (h_ @ ha_f) * (semantic_input @ hb_f)\n",
    "        preact_f = (preact_f @ hc_f.t()) + x_f + b_hf\n",
    "\n",
    "        preact_o = (h_ @ ha_o) * (semantic_input @ hb_o)\n",
    "        preact_o = (preact_o @ hc_o.t()) + x_o + b_ho\n",
    "\n",
    "        preact_c = (h_ @ ha_c) * (semantic_input @ hb_c)\n",
    "        preact_c = (preact_c @ hc_c.t()) + x_c + b_hc\n",
    "\n",
    "        i = torch.sigmoid(preact_i)\n",
    "        f = torch.sigmoid(preact_f)\n",
    "        o = torch.sigmoid(preact_o)\n",
    "        c = torch.tanh(preact_c)\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if 'bias' in self.__dict__ and self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
    "            s += ', nonlinearity={nonlinearity}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def check_forward_input(self, input):\n",
    "        if input.size(1) != self.input_size:\n",
    "            raise RuntimeError(\n",
    "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
    "                    input.size(1), self.input_size))\n",
    "\n",
    "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
    "        if input.size(0) != hx.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
    "                    input.size(0), hidden_label, hx.size(0)))\n",
    "\n",
    "        if hx.size(1) != self.hidden_size:\n",
    "            raise RuntimeError(\n",
    "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
    "                    hidden_label, hx.size(1), self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence is class for beam search\n",
    "class Sentence(object):\n",
    "    def __init__(self, max_nb_words, beam_width, end_word, vocab):\n",
    "        self.max_nb_words = max_nb_words\n",
    "        self.beam_width = beam_width\n",
    "        self.end_word = end_word\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.words = []\n",
    "        self.probability = 0\n",
    "        self.ended = False\n",
    "\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "\n",
    "    def update_words(self, s, state, y):\n",
    "        y = self.act(y)\n",
    "        new_s = []\n",
    "        for i in range(self.beam_width):\n",
    "            val, idx = y.max(dim=1)\n",
    "            y[0, idx] -= val\n",
    "            current_word = y.clone()\n",
    "            current_word[0,:] = 0\n",
    "            current_word[0,idx] = 1\n",
    "            s2 = s.copy()\n",
    "            s2.update_state(val, state, current_word)\n",
    "            new_s.append(s2)\n",
    "            if s2.ended:\n",
    "                break\n",
    "        return new_s\n",
    "\n",
    "    def update_state(self, p, state, current_word):\n",
    "        self.state = [s.clone() for s in state]\n",
    "        self.words.append(current_word)\n",
    "        self._update_probability(p)\n",
    "        self._update_finished()\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.state, self.words[-1]\n",
    "\n",
    "    def extract_sentence(self):\n",
    "        sentence = []\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            sentence.append(self.vocab.get_word(idx))\n",
    "        return [self.probability, sentence]\n",
    "\n",
    "    def _update_probability(self, p):\n",
    "        self.probability += log(p, 2)\n",
    "\n",
    "    def _update_finished(self):\n",
    "        n = len(self.words)\n",
    "        f = self.words[-1]\n",
    "        if (n > self.max_nb_words) or (f == self.end_word).all():\n",
    "            self.ended = True\n",
    "\n",
    "    def copy(self):\n",
    "        new = Sentence(self.max_nb_words,\n",
    "                       self.beam_width,\n",
    "                       self.end_word,\n",
    "                       self.vocab)\n",
    "        new.words = [w.clone() for w in self.words]\n",
    "        new.probability = self.probability\n",
    "        return new\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.probability < other.probability\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            s += \"{}, \".format(self.vocab.get_word(idx))\n",
    "        return s\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, beam_width):\n",
    "        self.beam_width = beam_width\n",
    "        self.heap = []\n",
    "\n",
    "    def push(self, s):\n",
    "        s.probability *= -1\n",
    "        heapq.heappush(self.heap, s)\n",
    "\n",
    "    def pop(self):\n",
    "        s = heapq.heappop(self.heap)\n",
    "        s.probability *= -1\n",
    "        return s\n",
    "\n",
    "    def trim(self):\n",
    "        h2 = []\n",
    "        for i in range(self.beam_width):\n",
    "            if len(self.heap) == 0:\n",
    "                break\n",
    "            heapq.heappush(h2, heapq.heappop(self.heap))\n",
    "        self.heap=h2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictWord(nn.Module):\n",
    "    def __init__(self, dim_language_lstm, dict_size):\n",
    "        super(PredictWord, self).__init__()\n",
    "        self.fc = nn.Linear(dim_language_lstm, dict_size)\n",
    "        \n",
    "    def forward(self, h2):\n",
    "        y = self.fc(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zeros(shape, cuda=False):\n",
    "    zeros = torch.zeros(shape)\n",
    "    if cuda:\n",
    "        zeros = zeros.cuda()\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullProductDataset(Dataset):\n",
    "    \"\"\"Dataset for product with it's image, title token, and semantic input.\"\"\"\n",
    "\n",
    "    def __init__(self, file_names, semantic_inputs, titles, image_dir, vocab, transformer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.file_names = file_names\n",
    "        self.semantic_inputs = semantic_inputs\n",
    "        self.titles = titles\n",
    "        self.image_dir = image_dir\n",
    "        self.transformer = transformer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.file_names[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        \n",
    "        semantic_input = self.semantic_inputs[idx]\n",
    "        semantic_input = torch.tensor(semantic_input, dtype=torch.float)\n",
    "        \n",
    "        title = [vocab(token) for token in self.titles[idx]]\n",
    "        title = torch.tensor(title)\n",
    "     \n",
    "        return image, semantic_input, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(sample):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - feature: torch tensor of shape (36,2048).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        features: torch tensor of shape (batch_size, 36, 2048).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort data list by caption length (descending order).\n",
    "    sample.sort(key=lambda x: len(x[2]), reverse=True)\n",
    "    images, semantic_inputs, titles = zip(*sample)\n",
    "    \n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(title) for title in titles]\n",
    "\n",
    "    targets = torch.zeros(len(titles), max(lengths)).long()\n",
    "    for i, title in enumerate(titles):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = title[:end]\n",
    "\n",
    "    return torch.stack(images), torch.stack(semantic_inputs), targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "dataset = FullProductDataset(file_names_train, semantic_inputs_train, labels_train, \"data/images\", vocab)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, semantic_input, title = dataset[i]\n",
    "\n",
    "    print(i, image.size, semantic_input.shape, title.shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformer = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoder Model from Image Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTagger(nn.Module):\n",
    "    r\"\"\"Tagger Encoder extends ResNet152 Model.\n",
    "\n",
    "    Arguments:\n",
    "        semantic_size (int, optional): size of semantic size\n",
    "        dropout (float, optional): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, semantic_size=1000, dropout=0.15):\n",
    "        super(ImageTagger, self).__init__()\n",
    "        self.semantic_size = semantic_size\n",
    "\n",
    "        # Using pre-trained ImageNet\n",
    "        resnet = torchvision.models.resnet152(pretrained=True)\n",
    "\n",
    "        # Remove linear layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(2048, semantic_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        r\"\"\"Forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            images (torch.Tensor): images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        Returns \n",
    "            torch.Tensor: probabilites of tags (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.view(out.size(0), -1)   # (batch_size, 2048)\n",
    "        out = self.dropout(out)           # (batch_size, 2048)\n",
    "        out = self.linear(out)            # (batch_size, 1000)\n",
    "        out = self.sigmoid(out)           # (batch_size, 1000)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        r\"\"\"Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "        Arguments\n",
    "            fine_tune (boolean): Allow fine tuning?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tagger = ImageTagger()\n",
    "image_tagger = image_tagger.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tagger.load_state_dict(torch.load(\"models/image_tagger.best_1\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = image_tagger.resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dict_size, image_feature_dim, vocab, tf_ratio):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.dict_size = dict_size\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.vocab = vocab\n",
    "        self.tf_ratio = tf_ratio\n",
    "\n",
    "        self.embed_word = nn.Linear(dict_size, WORD_EMB_DIM, bias=False)\n",
    "        self.lstm1 = AttentionLSTM(WORD_EMB_DIM,\n",
    "                                   NB_HIDDEN_LSTM2,\n",
    "                                   image_feature_dim,\n",
    "                                   NB_HIDDEN_LSTM1)\n",
    "        self.lstm2 = SCNCell(NB_HIDDEN_LSTM1+image_feature_dim,\n",
    "                             NB_HIDDEN_LSTM2,\n",
    "                             SEMANTIC_DIM,\n",
    "                             FACTORED_DIM,\n",
    "                             bias=True)\n",
    "        self.attention = Attend(image_feature_dim,\n",
    "                                NB_HIDDEN_LSTM1,\n",
    "                                NB_HIDDEN_ATT)\n",
    "        self.predict_word = PredictWord(NB_HIDDEN_LSTM2, dict_size)\n",
    "\n",
    "        self.h1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.c1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.h2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "        self.c2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "    \n",
    "    def forward(self, image_feats, nb_timesteps, semantic_input, true_words, beam=None):\n",
    "        if beam is not None:\n",
    "            return self.beam_search(image_feats, nb_timesteps, beam, semantic_input)\n",
    "\n",
    "        nb_batch, nb_image_feats, _ = image_feats.size()\n",
    "        use_cuda = image_feats.is_cuda\n",
    "\n",
    "        v_mean = image_feats.mean(dim=1)\n",
    "\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "        y_out = make_zeros((nb_batch, nb_timesteps-1, self.dict_size), cuda = use_cuda)\n",
    "\n",
    "        for t in range(nb_timesteps-1):\n",
    "            y, state = self.forward_one_step(state,\n",
    "                                             current_word,\n",
    "                                             v_mean,\n",
    "                                             image_feats,\n",
    "                                             semantic_input)\n",
    "            y_out[:,t,:] = y\n",
    "\n",
    "            current_word = self.update_current_word(y, true_words, t, use_cuda)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "    def forward_one_step(self, state, current_word, v_mean, image_feats, semantic_input):\n",
    "        h1, c1, h2, c2 = state\n",
    "        word_emb = self.embed_word(current_word)\n",
    "        h1, c1 = self.lstm1(h1, c1, h2, v_mean, word_emb)\n",
    "        v_hat = self.attention(image_feats, h1)\n",
    "        h2, c2 = self.lstm2(h1, v_hat, semantic_input)\n",
    "        y = self.predict_word(h2)\n",
    "        state = [h1, c1, h2, c2]\n",
    "        \n",
    "        return y, state\n",
    "\n",
    "    def update_current_word(self, y, true_words, t, cuda):\n",
    "        use_tf = True if random.random() < self.tf_ratio else False\n",
    "        if use_tf:\n",
    "            next_word = true_words[:,t+1]\n",
    "        else:\n",
    "            next_word = torch.argmax(y, dim=1)\n",
    "\n",
    "        current_word = indexto1hot(len(self.vocab), next_word)\n",
    "        current_word = torch.from_numpy(current_word).float()\n",
    "        \n",
    "        if cuda:\n",
    "            current_word = current_word.cuda()\n",
    "    \n",
    "        return current_word\n",
    "\n",
    "    def init_inference(self, nb_batch, cuda):\n",
    "        start_word = indexto1hot(len(self.vocab), self.vocab('<start>'))\n",
    "        start_word = torch.from_numpy(start_word).float().unsqueeze(0)\n",
    "        start_word = start_word.repeat(nb_batch, 1)\n",
    "\n",
    "        if cuda:\n",
    "            start_word = start_word.cuda()\n",
    "\n",
    "        h1 = self.h1.repeat(nb_batch, 1)\n",
    "        c1 = self.c1.repeat(nb_batch, 1)\n",
    "        h2 = self.h2.repeat(nb_batch, 1)\n",
    "        c2 = self.c2.repeat(nb_batch, 1)\n",
    "        state = [h1, c1, h2, c2]\n",
    "\n",
    "        return state, start_word\n",
    "\n",
    "    #########################################\n",
    "    #               BEAM SEARCH             #\n",
    "    #########################################\n",
    "    def beam_search(self, image_features, max_nb_words, beam_width, semantic_input):\n",
    "        # Initialize model\n",
    "        use_cuda = image_features.is_cuda\n",
    "        nb_batch, nb_image_feats, _ = image_features.size()\n",
    "\n",
    "        v_mean = image_features.mean(dim=1)\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "\n",
    "        # Initialize beam search\n",
    "        end_word = indexto1hot(len(self.vocab), self.vocab('<end>'))\n",
    "        end_word = torch.from_numpy(end_word).float().unsqueeze(0)\n",
    "        end_word = end_word.cuda() if image_features.is_cuda else end_word\n",
    "        beam = Beam(beam_width)\n",
    "        s = Sentence(max_nb_words, beam_width, end_word, self.vocab)\n",
    "        s.update_state(1.0, state, current_word)\n",
    "        beam.push(s)\n",
    "\n",
    "        # Perform beam search\n",
    "        final_beam = Beam(beam_width)\n",
    "        while len(beam) > 0:\n",
    "            s = beam.pop()\n",
    "            new_sentences = self.update_states(s, image_features, v_mean, semantic_input)\n",
    "            for s in new_sentences:\n",
    "                if s.ended:\n",
    "                    final_beam.push(s)\n",
    "                else:\n",
    "                    beam.push(s)\n",
    "            # Get rid of low scoring sentences\n",
    "            beam.trim() \n",
    "            final_beam.trim()\n",
    "\n",
    "        # Extract final sentence\n",
    "        s = final_beam.pop() # Best sentence on top of heap\n",
    "        sentence = s.extract_sentence()\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def update_states(self, s, image_feats, v_mean, semantic_input):\n",
    "        state, current_word = s.get_states()\n",
    "        y, state = self.forward_one_step(state, current_word, v_mean, image_feats, semantic_input)\n",
    "        y = self.remove_consecutive_words(y, current_word)\n",
    "        new_sentences = s.update_words(s, state, y)\n",
    "        return new_sentences\n",
    "\n",
    "    def remove_consecutive_words(self, y, prev_word):\n",
    "        # give previous word low score so as not to repeat words\n",
    "        y = y - 10**10 * prev_word\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMB_DIM = 1000\n",
    "NB_HIDDEN_LSTM1 = 1000\n",
    "NB_HIDDEN_LSTM2 = 1000\n",
    "NB_HIDDEN_ATT = 512\n",
    "\n",
    "IMAGE_FEATURE_DIM = 2048\n",
    "FACTORED_DIM = 512\n",
    "SEMANTIC_DIM = 1000\n",
    "\n",
    "TF_RATIO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = CaptionModel(len(vocab), IMAGE_FEATURE_DIM, vocab, TF_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = caption_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FullProductDataset(file_names_train, semantic_inputs_train, labels_train, \"data/images\", vocab, image_transformer)\n",
    "val_dataset = FullProductDataset(file_names_val, semantic_inputs_val, labels_val, \"data/images\", vocab, image_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(reference, hypothesis, ngram):\n",
    "    weights = [0, 0, 0, 0]\n",
    "    weights[0:ngram] = [1/ngram for i in range(ngram)]\n",
    "    \n",
    "    return sentence_bleu([reference], hypothesis, weights=tuple(weights), auto_reweigh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, encoder, train_loader, optimizer, kbar):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (images, semantic_inputs, captions, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        semantic_inputs = semantic_inputs.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # forward\n",
    "        image_feats = encoder(images)\n",
    "        image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "        outputs = model(image_feats, len(captions[0]), semantic_inputs, captions)\n",
    "\n",
    "        captions = captions[:, 1:]\n",
    "\n",
    "        decode_lengths = [x-1 for x in lengths]\n",
    "        captions, _, _, _ = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
    "        outputs, _, _, _ = pack_padded_sequence(outputs, decode_lengths, batch_first = True)\n",
    "\n",
    "        batch_loss = loss(outputs, captions)\n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        kbar.update(i, values=[(\"loss\", batch_loss)])\n",
    "        \n",
    "    epoch_loss = epoch_loss / nb_batch \n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model, val_loader, vocab, beam=None):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    bleu3_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, semantic_inputs, captions, lengths) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            semantic_inputs = semantic_inputs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            image_feats = encoder(images)\n",
    "            image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "            \n",
    "            if beam is not None and (i % 200) == 0:\n",
    "                sentences = model(image_feats, 20, semantic_inputs, beam)\n",
    "                print(sentences)\n",
    "\n",
    "            outputs = model(image_feats, len(captions[0]), semantic_inputs, captions)\n",
    "\n",
    "            for j, (output, caption) in enumerate(zip(outputs, captions)):\n",
    "                caption_list = [vocab.get_word(wid.item()) for wid in caption if wid != 0]                \n",
    "                caption_list = caption_list[1:-1]\n",
    "\n",
    "                padded_result = [vocab.get_word(torch.argmax(one_hot_en).item()) for one_hot_en in output]\n",
    "                result_list = []\n",
    "                for word in padded_result:\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    result_list.append(word)\n",
    "                    \n",
    "                bleu3_scores.append(bleu_score(caption_list, result_list, 3))\n",
    "            \n",
    "            decode_lengths = [x-1 for x in lengths]\n",
    "            captions, _, _, _ = pack_padded_sequence(captions, decode_lengths, batch_first=True)\n",
    "            outputs, _, _, _ = pack_padded_sequence(outputs, decode_lengths, batch_first = True)\n",
    "\n",
    "            batch_loss = loss(outputs, captions)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            \n",
    "    bleu3_score_avg = sum(bleu3_scores)/len(bleu3_scores)\n",
    "    epoch_loss = epoch_loss / nb_batch \n",
    "\n",
    "    return bleu3_score_avg, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab, optimizer, scheduler, max_epochs, current_epoch=0):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loss_array = []\n",
    "    val_loss_array = []\n",
    "    val_bleu3_array = []\n",
    "\n",
    "    # some big number\n",
    "    min_val_loss = 10**5\n",
    "    max_bleu3_score = 0\n",
    "    train_epoch_array = []\n",
    "    val_epoch_array = []\n",
    "    \n",
    "#     if current_epoch == 0:\n",
    "#         bleu3_score, val_loss = val_one_epoch(model, val_loader, vocab, beam=None)\n",
    "#         print(\"Validation loss with random initialization. Loss: \" + str(val_loss) + \", BLEU3 score: \" + str(bleu3_score))\n",
    "    \n",
    "    while current_epoch < max_epochs:\n",
    "        current_epoch += 1\n",
    "        \n",
    "        print('Epoch {}/{}'.format(current_epoch, max_epochs))\n",
    "        kbar = pkbar.Kbar(target=len(train_loader), width=32)\n",
    "        \n",
    "        train_loss = train_one_epoch(model, encoder, train_loader, optimizer, kbar)\n",
    "        bleu3_score, val_loss = val_one_epoch(model, val_loader, vocab, beam=None)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, \"models/caption.epoch_\" + str(current_epoch))\n",
    "\n",
    "        kbar.add(1, values=[(\"loss\", train_loss), (\"val_loss\", val_loss), (\"BLEU3_score\", bleu3_score)])\n",
    "        \n",
    "        train_loss_array.append(train_loss)\n",
    "        val_loss_array.append(val_loss)\n",
    "        val_bleu3_array.append(bleu3_score)\n",
    "        train_epoch_array.append(current_epoch)\n",
    "        val_epoch_array.append(current_epoch)\n",
    "        \n",
    "        # keep track of the best model and save it\n",
    "        if bleu3_score > max_bleu3_score:\n",
    "            max_bleu3_score = bleu3_score\n",
    "            torch.save({\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, \"models/caption.best_bleu3\")\n",
    "            \n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, \"models/caption.best_loss\")\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\n')\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(min_val_loss))\n",
    "    print('Best val BLEU3 score: {:4f}'.format(max_bleu3_score))\n",
    "    \n",
    "    return model, train_loss_array, val_loss_array, val_bleu3_array, train_epoch_array, val_epoch_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lr = 5e-5\n",
    "optimizer = optim.Adam(caption_model.parameters(), lr=lr)\n",
    "epochs = 28\n",
    "# optimizer = optim.SGD(model.parameters(),\n",
    "#                       lr=args.lr,\n",
    "#                       momentum = 0.899999976158,\n",
    "#                       weight_decay=0.000500000023749)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "checkpoint = torch.load(\"models/caption.best_bleu3_1\")\n",
    "caption_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "current_epoch = checkpoint['epoch']\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model, train_loss_array, val_loss_array, val_bleu3_array, train_epoch_array, val_epoch_array = train(\n",
    "    caption_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    current_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_bleu3_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhV5bX48e8iI2RiSIBAAoFAGGQ2ghOTMjhQELSKV69T1TrVqejVX+29tdZrq4jUinBxwOvQIlrxoq2CgIBaFYKCMgUCBAnzYEgCmbN+f5ydeAgnE5yTnWF9nicPZw/v3mtvzjnrvPt9935FVTHGGGP8oYXbARhjjGk6LKkYY4zxG0sqxhhj/MaSijHGGL+xpGKMMcZvgt0OwE2xsbGalJTkdhjGGNOorF279rCqxvla1qyTSlJSEmlpaW6HYYwxjYqI7KpqmV3+MsYY4zeWVIwxxviNJRVjjDF+06zbVHwpLi4mKyuLgoICt0MxTUh4eDgJCQmEhIS4HYoxAWVJpZKsrCyioqJISkpCRNwOxzQBqsqRI0fIysqiW7dubodjTEDZ5a9KCgoKaNeunSUU4zciQrt27az2a5oFSyo+WEIx/mbvKdNc2OUvY0yjpKoczC1k074ctu7PpU1EKIMSW5McF0lQC0vibrGaSgMzatQoFi9efNK8mTNnctddd1Vbpvwmzssuu4zs7OxT1vnd737H9OnTq933+++/z6ZNmyqm//M//5OlS5fWJXyfVqxYQUxMDIMGDWLAgAGMGTOGgwcPAvDaa69xzz33nFImKSmJ/v37M2jQIAYNGsS9994LnHysAJmZmfTr1w+A1atXV6w/cOBAFi5ceMaxm4ahoLiUDXuO8U7abn7/wSb+7aWvGPLEJwz772XcPG8NT320hYff/Y5xz61iwO8WM3Xulzz10WY++n4fe7PzsXGj6k9AayoicgnwZyAIeFlV/1hp+STgCaAMKAHuV9XPvZYHAWnAHlWd4MwbBMwBwp0yd6nqamfZo8AvgFLgXlU9+du5Ebj22muZP38+48ePr5g3f/58nnnmmVqV/+c//3na+37//feZMGECffv2BeD3v//9aW+rsuHDh/Phhx8C8OijjzJr1iwef/zxast8+umnxMbG1nof/fr1Iy0tjeDgYPbt28fAgQP52c9+RnBw4N7mqoqq0qKF/T7zB1XlQE4hm/fnsHlfDlv25bJ5Xw47Dh+ntMyTGMJDWtCrQxTjz+pI745R9ImPplfHKI4cL2L97mzW785mXdYx5n2eSVFpGQBxUWEMSmzNoMTWDExoTf+EGGJaWk+8QAjYp81JCLOAsUAWsEZEFqnqJq/VlgGLVFVFZACwAOjttfw+YDMQ7TXvaeBxVf1IRC5zpkeJSF9gKnAW0AlYKiIpqloaoEMMiKuuuorHHnuMwsJCwsLCyMzMZO/evVx44YXceeedrFmzhvz8fK666iqfX8rlj56JjY3lySef5PXXXycxMZG4uDjOPvtsAF566SXmzp1LUVERPXr04I033mDdunUsWrSIlStX8oc//IG///3vPPHEE0yYMIGrrrqKZcuWMW3aNEpKSjjnnHOYPXs2YWFhJCUlceONN/LBBx9QXFzMO++8Q+/evU+Jq5yqkpubS48ePfx+7lq1alXxuqCgoMp2jEceeYRFixYRHBzMuHHjmD59OgcOHOCOO+5gx44dAMyePZvzzz+fGTNm8OqrrwJw6623cv/995OZmcmll17K6NGj+fLLL3n//fdZsGABCxYsoLCwkMmTJ9eYMI2n9pFxMI9NXsljy/4cfjxRXLFO59Yt6RPvSSB94qPpHR9FUrsIn5e3WrcKJTkukilDEgAoLCll875cr0STzSebDlSs3z0ugkEJrRmY6PnrEx9FWHBQ4A+8iQtkTWUokKGqOwBEZD4wCahIKqqa57V+BFBRRxWRBOBy4EngQa/1lJ+STAyw13k9CZivqoXAThHJcGL48nQP4PEPNrJpb87pFvepb6do/utnZ1W5vF27dgwdOpSPP/6YSZMmMX/+fK655hpEhCeffJK2bdtSWlrKxRdfzHfffceAAQN8bmft2rXMnz+fb7/9lpKSEoYMGVKRVKZMmcJtt90GwGOPPcYrr7zCr371KyZOnFiRRLwVFBRw0003sWzZMlJSUrjhhhuYPXs2999/PwCxsbF88803vPjii0yfPp2XX375lHg+++wzBg0axJEjR4iIiOC///u/azxXo0ePJijI8yG/8cYbeeCBB2os8/XXX3PLLbewa9cu3njjjVNqKUePHmXhwoVs2bIFEam4VHjvvfcycuRIFi5cSGlpKXl5eaxdu5Z58+bx9ddfo6oMGzaMkSNH0qZNG9LT05k3bx4vvvgiS5YsYdu2baxevRpVZeLEiaxatYoRI0bUGG9zUFH72Jfj1EBy2eKr9tEx+qfk0TGK3vHRZ1SbCAsOqqidlDuWX8z3WcdYn5XNut3ZfJZxmPe+3QNASJDQNz7ak2ScZNM9NoIW1j5TJ4FMKp2B3V7TWcCwyiuJyGTgKaA9niRSbibwMBBVqcj9wGIRmY6nTeh8r/19VWl/nX3s73bgdoAuXbrU/mjqUfklsPKkUv5LecGCBcydO5eSkhL27dvHpk2bqkwqn332GZMnT6749T5x4sSKZRs2bOCxxx4jOzubvLy8ky61+ZKenk63bt1ISUkBPF/ws2bNqkgqU6ZMAeDss8/mvffe87kN78tff/rTn3j44YeZM2dOtfv1dfnLV+3De96wYcPYuHEjmzdv5sYbb+TSSy8lPDy8Ynl0dDTh4eHceuutXH755UyYMAGA5cuX8/rrrwMQFBRETEwMn3/+OZMnTyYiIqLiOD/77DMmTpxI165dOffccwFYsmQJS5YsYfDgwQDk5eWxbdu2ZplUCopL2XYg7+TLV/tzyPZR+7ikX0d6d4ymT3wUXauoffhbTMsQLuwZy4U9Pe8rVWV/ToGnJrP7GOt3Z/P3tVm8/qXneYlRYcEMSIypSDKDE1vTPjq8ul00e4FMKr7eIae0lqnqQmChiIzA074yRkQmAAdVda2IjKpU5E7gAVX9u4hcDbwCjKnD/uYCcwFSU1Orbb2rrkYRSFdccQUPPvgg33zzDfn5+QwZMoSdO3cyffp01qxZQ5s2bbjppptqvO+hqss/N910E++//z4DBw7ktddeY8WKFdVup6ZGzrCwMMDzZVxSUlLtuuBJcFdeeWWN6/nSrl07fvzxx4rpo0eP+mx36dOnDxEREWzYsIHU1NSK+cHBwaxevZply5Yxf/58XnjhBZYvX+5zX9Udd3miKV/v0Ucf5Ze//OXpHFKjVP5lvGVfrufy1X7P5audXrWPliFBpHSM4tKK5OFp+2hIbRkiQnxMS+JjWnJJv3gASsuU7YfyWOdcNluflc3cVTsocY4rPia8IskMTIyhf+cYosIbzjG5LZBJJQtI9JpO4KdLVadQ1VUikiwiscAFwESnzSQciBaRN1X1euBGPG0tAO8A5dda6rS/hiwyMpJRo0Zxyy23cO211wKQk5NDREQEMTExHDhwgI8++ohRo0ZVuY0RI0Zw00038cgjj1BSUsIHH3xQ8aWXm5tLfHw8xcXFvPXWW3Tu7KnQRUVFkZube8q2evfuTWZmJhkZGRVtMCNHjjzt4/v8889JTk4+rbKjRo3izTffZMyYMYgI//u//8vo0aMB2LlzJ4mJiQQHB7Nr1y7S09OpPF5OXl4eJ06c4LLLLuPcc8+taNu5+OKLKy7plZaWcvz48ZPOoaqycOFC3njjjVNiGj9+PL/97W+57rrriIyMZM+ePYSEhNC+ffvTOsaGpqL2UXH5ypNEvGsfCW1a0rtjNJf160hv5/JVfdU+/C2ohZDSIYqUDlFcner5SikoLmXj3pyKJLN+dzYfb9wPgAj0iIusaJsZlNCaXh2jCA1unp03AplU1gA9RaQbsAdPI/q/ea8gIj2A7U5D/RAgFDiiqo8CjzrrjAKmOQkFPIliJLACuAjY5sxfBPxVRGbgaajvCawO2NEF2LXXXsuUKVOYP38+AAMHDmTw4MGcddZZdO/enQsuuKDa8kOGDOGaa65h0KBBdO3aleHDh1cse+KJJxg2bBhdu3alf//+FYlk6tSp3HbbbTz//PO8++67FeuHh4czb948fv7zn1c01N9xxx11Op7yNhVVJSYm5qR2l9dee43333+/YvqrrzxXMb3bVAYMGMDrr7/O7bffzpYtWxg4cCAiQmpqKk899RTgSVZ//OMfCQkJoUWLFrz44oun1GJyc3OZNGkSBQUFqCrPPfccAH/+85+5/fbbeeWVVwgKCmL27Nmcd9553HTTTQwdOhTwNNQPHjyYzMzMk7Y5btw4Nm/ezHnnnQd4fhS8+eabjS6pqCr7jhWwxWn3KE8eOw7l4fxIp2VIEL06RnFpv3j6xP/U8yq6if9SDw8J4uyubTi7a5uKeT8eL+K7PccqOgJ8uuUg767NAiA0uAVndYpmYILT4yyxNUntWjWLm2AlkP23nZrGTDxdil9V1SdF5A4AVZ0jIv8B3AAUA/nAQ95dip1tjMKTVMq7FF+Ip5tyMFCAp0vxWmfZb4Bb+Kl78kfVxZeamqqVB+navHkzffr0OaPjNsaXhvTeKiguZesBT+LwTiDH8k+uffSJj6aP0223d3w0Xdu2sobrKqgqe7Lzf7pstvsY3+85Rn6xpwNqTMsQBiTEMNhJMgMSWhMXFeZy1KdHRNaqaqrPZc35piBLKqY+ufHeKq99lCcNT/ddT9tHee2jVain9tG7YzR94z29rppD7aM+lJSWse1gXsVls3W7j5G+P6fi3Hdu3dKpyXg6A/TrHENEWMN/0El1SaXhR2+MqZWC4lLS9+eecvnKu/aR2NbT9nH5gE4VNZAuVvsImOCgFp7aXnw0U4d6epueKCqpaJ9Z5ySbf3y/D4AWAikdok7qCNCrQxTBQY2nfcaSig+q2iyufZr6488rAqrK3mMFbNmX4zSeexJIpo/ax+UD4iuSR4rVPhqEVqHBnJPUlnOS2lbMO5JXWFGTWb87m8Wb9vN2mueOjPCQFvTrFFPREWBwYmsS2rRssN9Rdvmr0uWvnTt3EhUVZY+/N35TPp5Kbm5uncdTyS/6qe3D+/JVTsFPXbcT27akT0dPm0ffeM9lLKt9NG6qyg9HTzjtM56bNTfsOUZhieexM20jQhmY8FOiGZjQmrYRofUWn7WpVMFXUrGRH00g1GXkx9Iy5Q//2MTKrYdOqX2U32le3oDeq2OU3SPRTBSXlpG+P7eiS/P63cfYejCX8q/wLm1bOQkmhkGJrTmrUwwtQwPz2BlLKlXwlVSMcdvCb7N44O31jEiJY7DzTKo+8dEktrHahzlZXmFJxWNnyrs27z3m+UEc1ELo1SHKc+9MoqdW07N9lF/uHbKkUgVLKqahKSop4+IZK4gKC+HDX11oScTU2cGcAtZnHfPqcZZNrnO5tFVoEP06e2oyF/aIZURK3Gntw3p/GdNIvL3mB3YfzWfezf0soZjT0j46nLF9wxnbtwMAZWVK5pHjTm3mGOt2Z/PaF5kczi087aRSHUsqxjQQJ4pKeH55BkOT2jIqAB920zy1aCF0j4uke1wkkwf/NCzA8cLAjApiScWYBmLeF5kcyi1k9nVDrOehCaiw4KCAjR3TeO6oMaYJO3aimP9ZuZ2Le7cn1ev+BWMaG0sqxjQAc1ZtJ7ewhGnje7kdijFnxJKKMS47mFPAvC92MnFgJ/rER9dcwJgGzJKKMS77y/IMSkqVB8emuB2KMWfMkooxLtp15Dh/W/0DU4cm0rVdRM0FjGngLKkY46LnPtlKcJBw70U93Q7FGL+wpGKMS7bsz+H/1u/lpvO70T463O1wjPELSyrGuGT64nQiw4K5c2Sy26EY4zeWVIxxQVrmUZZuPsgdI5OJaWVPGTZNhyUVY+qZqvL04nRiI8O4+YIkt8Mxxq8sqRhTz1ZuPcTqnUe59+IetAq1JyWZpsWSijH1qKxMeWZxOoltWzL1nC5uh2OM31lSMaYe/eP7fWzcm8MDY1IIDbaPn2l67F1tTD0pLi1jxidb6dUhikmDOrsdjjEBYUnFmHry7tosdh4+zrTxvfwypKsxDZElFWPqQUFxKX9euo0hXVozpk97t8MxJmAsqRhTD17/MpP9OQU8NL63DcBlmjRLKsYEWE5BMS+u2M6IlDjOS27ndjjGBJQlFWMC7OVVO8g+UczDNgCXaQYsqRgTQIfzCnn5851c3j+efp1j3A7HmICzpGJMAL2wPIPCkjIeHGcDcJnmwZKKMQGS9eMJ/vr1D/z87ASS4yLdDseYemFJxZgAmbl0GwjcN8YG4DLNhyUVYwJg24Fc3vsmixvO7Up8TEu3wzGm3lhSMSYAnl2ylVahwdw1uofboRhTryypGONn63Zn8/HG/dw2vDttI0LdDseYemVJxRg/e2bxFtpFhPKL4d3cDsWYemdJxRg/+iLjMF9kHOGu0T2IDLMBuEzzY0nFGD9RVZ7+eAudYsK5bpgNwGWaJ0sqxvjJ4o37WZ91jPvHpBAeEuR2OMa4IqBJRUQuEZF0EckQkUd8LJ8kIt+JyDoRSRORCystDxKRb0XkQ695bzvrrxORTBFZ58xPEpF8r2VzAnlsxngrLVOmL9lKclwEU4bYAFym+QrYRV8RCQJmAWOBLGCNiCxS1U1eqy0DFqmqisgAYAHQ22v5fcBmILp8hqpe47WPZ4FjXutvV9VBfj8YY2rw3jdZZBzMY/Z1QwgOsgsApvkK5Lt/KJChqjtUtQiYD0zyXkFV81RVnckIoPw1IpIAXA687Gvj4hmU4mrgbwGI3ZhaKywpZebSbQxIiOGSfh3dDscYVwUyqXQGdntNZznzTiIik0VkC/AP4BavRTOBh4GyKrY/HDigqtu85nVzLpetFJHhvgqJyO3Opba0Q4cO1eFwjPHtra9+YE92Pg+N72UDcJlmL5BJxdenS0+ZobpQVXsDVwBPAIjIBOCgqq6tZvvXcnItZR/QRVUHAw8CfxWR6MqFVHWuqqaqampcXFztj8YYH/IKS5j1aQbnJ7fjwh6xbodjjOsCmVSygESv6QRgb1Urq+oqIFlEYoELgIkikonnstlFIvJm+boiEgxMAd72Kl+oqkec12uB7YA9b9wE1Kuf7+TI8SKrpRjjCGRSWQP0FJFuIhIKTAUWea8gIj2cthFEZAgQChxR1UdVNUFVk5xyy1X1eq+iY4Atqprlta04p3MAItId6AnsCNzhmebu6PEiXlq1g3F9OzC4Sxu3wzGmQQhY7y9VLRGRe4DFQBDwqqpuFJE7nOVzgCuBG0SkGMgHrvFquK/OVE5toB8B/F5ESoBS4A5VPeqnwzHmFLNXZJBXVMI0GybYmApSu+/wpik1NVXT0tLcDsM0QvuO5TPymRX8bEAnnr16oNvhGFOvRGStqqb6WmYd6o05Dc8v24aqcr8NwGXMSSypGFNHOw7lsSAti+uGdSWxbSu3wzGmQbGkYkwdPfvJVsKCW3C3DcBlzCksqRhTBxv2HOMf3+3jFxd2Iy4qzO1wjGlwLKkYUwfPLE6ndasQbhvR3e1QjGmQLKkYU0tf7zjCyq2HuHNkMtHhIW6HY0yDZEnFmFpQVZ5enE6H6DBuPD/J7XCMabAsqRhTC8s2H2Ttrh+59+KeNgCXMdWwpGJMDcrKlOlL0klq14qrUxNrLmBMM2ZJxZgaLFq/ly37c3lwXC9CbAAuY6plnxBjqlFUUsaMT7bSNz6aCf3j3Q7HmAbPkoox1Xh7zQ/8cPQED43vRYsW9mh7Y2pSY1IRkVYi8lsRecmZ7ukMomVMk3aiqITnl2cwNKkto3rZgG7G1EZtairzgELgPGc6C/hDwCIypoF47V+ZHMot5OFLbAAuY2qrNkklWVWfBooBVDUf30MFG9NkHDtRzJwV27mod3tSk9q6HY4xjUZtkkqRiLTEGV9eRJLx1FyMabLmrNpOTkEJ08bZAFzG1EVtRn78L+BjIFFE3sIzfvxNgQzKGDcdzClg3hc7mTSoE307RbsdjjGNSrVJRURaAG2AKcC5eC573aeqh+shNmNc8ZflGZSUKg+OTXE7FGManWqTiqqWicg9qroA+Ec9xWSMa344coK/rf6Ba85JpGu7CLfDMabRqU2byiciMk1EEkWkbflfwCMzxgUzPkknOEi492IbJtiY01GbNpVbnH/v9pqngA0oYZqULftz+L/1e/nliGQ6RIe7HY4xjVKNSUVVu9VHIMa4bfridCLDgrlzZLLboRjTaNWYVEQkBLgTGOHMWgH8j6oWBzAuY+rV2l1HWbr5IA+N70VMKxuAy5jTVZvLX7OBEOBFZ/rfnXm3BiooY+qTqvKnj9OJjQzj5guS3A7HmEatNknlHFUd6DW9XETWByogY+rbyq2HWL3zKI9PPItWobX5SBhjqlKb3l+lzl30AIhId6A0cCEZU3/KypRnFqeT0KYl1w7t4nY4xjR6tflZ9hDwqYjswHPzY1fg5oBGZUw9+eeGfWzcm8OMqwcSGmwjQRhzpmrT+2uZiPQEeuFJKltU1Z79ZRq94tIynl2ylZQOkUwa1NntcIxpEmoznsrdQEtV/U5V1wOtROSuwIdmTGC9uzaLnYePM21cL4JsAC5j/KI29f3bVDW7fEJVfwRuC1xIxgReQXEpf166jcFdWjO2bwe3wzGmyahNUmkhXiMUiUgQEBq4kIwJvDe+3MX+nAIeHt/bBuAyxo9q01C/GFggInPwPJ7lDjyPwjemUcopKObFFRkM7xnLecnt3A7HmCalNknlP4Db8dxVL8AS4OVABmVMIL28agc/nijm4fG93Q7FmCanNr2/yoA5wBzn6cQJqmr3qZhG6XBeIS9/vpPL+8fTPyHG7XCMaXJq0/trhYhEOwllHTBPRGYEPjRj/G/WpxkUlpTx4DgbgMuYQKhNQ32MqubgGf1xnqqeDYwJbFjG+F/Wjyd466sfuGpIAslxkW6HY0yTVJukEiwi8cDVwIcBjseYgJm5dBsI3DfGBuAyJlBqk1R+j6cHWIaqrnGe/bUtsGEZ41/bDuTy3jdZ3HBuVzq1bul2OMY0WbVpqH8HeMdregdwZSCDMsbfnl2ylVahwdw1uofboRjTpAX0CXoicomIpItIhog84mP5JBH5TkTWiUiaiFxYaXmQiHwrIh96zXvbWX+diGSKyDqvZY86+0oXkfGBPDbTeKzfnc3HG/dz6/ButI2w+3aNCaSADR7h3Hk/CxgLZAFrRGSRqm7yWm0ZsEhVVUQGAAsA75sH7gM2A9HlM1T1Gq99PAscc173BaYCZwGdgKUikmLdn83Ti7fQNiKUW4d3dzsUY5q8QNZUhuJph9mhqkXAfGCS9wqqmqeq6kxG4LljHwARSQAup4obLZ1Hx1wN/M2ZNQmYr6qFqroTyHBiMM3YFxmH+SLjCHeP7kFkmA3AZUygVZtURGSoiJzjvO4rIg+KyGW13HZnYLfXdJYzr/I+JovIFuAfwC1ei2YCDwNlVWx/OHBAVcs7DdR2f7c7l9rSDh06VMtDMY2RqvL04nQ6xYRz3TAbgMuY+lBlUhGR/wKeB2aLyFPAC0Ak8IiI/KYW2/b1lD49ZYbqQlXtDVwBPOHsewJwUFXXVrP9a/mpllKX/c1V1VRVTY2Li6suftPILd54gPW7s7l/TArhIUFuh2NMs1Dd9YCrgEFAGLAfz+NZckTkGeBr4Mkatp0FJHpNJwB7q1pZVVeJSLKIxAIXABOdWlE4EC0ib6rq9QAiEoznZsyzT3d/pmkrLVOmL0knOS6CKUNsAC5j6kt1l79KVLVUVU8A25276lHVfKq+JOVtDdBTRLqJSCieRvRF3iuISI/yx+qLyBA8j9Q/oqqPqmqCqiY55ZaXJxTHGDwjUGZ5zVsETBWRMBHpBvQEVtciTtMEvfdNFhkH8/j1uF4EB9kwwcbUl+pqKkUi0spJKhU1AhGJoRZJRVVLROQePDdOBgGvqupGEbnDWT4Hz/0uN4hIMZAPXOPVcF+dqZx86Qtn2wuATUAJcLf1/GqeCktKmbl0G/07x3Bpv45uh2NMsyJVfYeLSJivseidy1Pxqvp9oIMLtNTUVE1LS3M7DONn877YyeMfbOKNXwxleE9rNzPG30Rkraqm+lpWXU0lQkQivKYVyFbVw8BhfwZojL/kFZbwwvIMzuvejgt7xLodjjHNTnVJZS2eROLdqypSRNYDt6pqZiADM+Z0vPr5To4cL+KhS3rZMMHGuKDKpKKq3XzNF5EpeAbtuiRQQRlzOn48XsRLq3Ywrm8HhnRp43Y4xjRLde4Wo6rvAe0DEIsxZ2T2yu3kFZUwbXwvt0Mxptmqc1IRkcjTKWdMIO07ls///iuTyYM7k9Ihyu1wjGm2qrz8JSIP+pjdBpiI5+56YxqM55dto0yVB8bYMMHGuKm6hvrKP/cUz5311zeF7sSm6dhxKI8FaVn8+7ldSWzbyu1wjGnWqmuof7yqZSISrKolgQnJmLqZ8clWwoJbcLcNwGWM66p7oOTnXq/fqLTYHn9iGoQNe47x4Xf7uOWCbsRFhbkdjjHNXnUN7t43PvartMxuADANwjOL04lpGcJtI2wALmMaguqSilbx2te0MfXu6x1HWLn1EHeNSiamZYjb4RhjqL6hvrWITMaTeFo7Nz2Cp5YSE/DIjKlG+QBcHaLDuPH8JLfDMcY4qksqK/F0Hy5//TOvZasCFpExtbB8y0HW7vqRJyf3swG4jGlAquv9dXNVy0TkysCEY0zNysqUZxank9SuFVenJtZcwBhTb073zvjn/BqFMXWwaP1etuzP5YGxKYTYAFzGNCin+4m03l/GFUUlZcz4ZCt94qP52YBObodjjKnkdJOK9f4yrng7bTc/HD3Bw+N70aKF/bYxpqGp7tlf3+M7eQjQIWARGVOF/KJSnl+2jXOS2jCql43oaExDVF3vrwn1FoUxtTDvXzs5lFvIi9cNsQG4jGmgquv9tavyPGd8+iNa1cD2xgTIsRPFzFmxnYt6t+ecpLZuh2OMqUJ1z/46V0RWiMh7IjJYRDYAG4ADImKjPpp69T+rtpNTUCx84aUAABJLSURBVMK0cTYAlzENWXWXv14A/h+eu+eXA5eq6lci0hv4G/BxPcRnDAdzCpj3RSYTB3aib6dot8MxxlSjut5fwaq6RFXfAfar6lcAqrqlfkIzxuMvyzMoLi3jwbE2AJcxDV11SaXM63V+pWXWpmLqxQ9HTvC31T9wzTmJJMVG1FzAGOOq6i5/DRSRHDxdiFs6r3GmwwMemTHAc0u3Ehwk3HtxT7dDMcbUQnW9v+wpfcZVW/bn8P66Pdw+ojsdou13jDGNgT04yTRY0xenExkWzJ0jk90OxRhTS5ZUTIO0dtdRlm4+yB0jk2ndKtTtcIwxtWRJxTQ4qsrTH6cTGxnGzRckuR2OMaYOLKmYBmfVtsN8vfMov7qoB61Cq+tLYoxpaCypmAalrEx5+uMtJLRpybVDu7gdjjGmjiypmAblnxv2sXFvDg+MSSE02N6exjQ29qk1DUZJaRkzlmwlpUMkVwzu7HY4xpjTYEnFNBjvrs1ix+HjTBvXiyAbgMuYRsmSimkQCopLmbl0G4O7tGZsXxsDzpjGypKKaRDe+HIX+3MKeGh8LxuAy5hGzJKKcV1uQTEvrshgeM9Yzk+OdTscY8wZsKRiXPfSZzv58UQxD4/v7XYoxpgzFNCkIiKXiEi6iGSIyCM+lk8Ske9EZJ2IpInIhZWWB4nItyLyYaX5v3K2u1FEnnbmJYlIvrOtdSIyJ5DHZvzjcF4hr3y2g8v6d6R/Qozb4RhjzlDAblcWkSBgFjAWyALWiMgiVd3ktdoyYJGqqogMABYA3j9X7wM2AxXD/YnIaGASMEBVC0Wkvdf621V1UGCOyATCrE8zyC8u5cGxNkywMU1BIGsqQ4EMVd2hqkXAfDzJoIKq5qlq+YBfEXgN/iUiCcDlwMuVtnsn8EdVLXS2cTBA8ZsAy/rxBG999QM/PzuRHu0j3Q7HGOMHgUwqnYHdXtNZzryTiMhkEdkC/AO4xWvRTOBhTh6BEiAFGC4iX4vIShE5x2tZN+dy2UoRGe4rKBG53bnUlnbo0KHTOCzjL39eug0E7htjA3AZ01QEMqn46hd6yjDEqrpQVXsDVwBPAIjIBOCgqq71sY1goA1wLvAQsEA8fVD3AV1UdTDwIPBXEYmuXFhV56pqqqqmxsXFneahmTOVcTCXv3+Txb+f25VOrVu6HY4xxk8CmVSygESv6QRgb1Urq+oqIFlEYoELgIkikonnstlFIvKm13bfU4/VeGoysapaqKpHnG2tBbbjqdWYBmj64q20DAnirlE2AJcxTUkgk8oaoKeIdBORUGAqsMh7BRHp4dQyEJEhQChwRFUfVdUEVU1yyi1X1eudYu8DFzllUpwyh0UkzukcgIh0B3oCOwJ4fOY0rd+dzccb93PbiO60iwxzOxxjjB8FrPeXqpaIyD3AYiAIeFVVN4rIHc7yOcCVwA0iUgzkA9d4NdxX5VXgVRHZABQBNzq9x0YAvxeREqAUuENVjwbm6MyZeGZxOm0jQrl1eHe3QzHG+JnU/B3edKWmpmpaWprbYTQrX2Qc5rqXv+axy/tYUjGmkRKRtaqa6muZ3VFv6o2q8vTidDrFhHP9uV3dDscYEwCWVEy9WbzxAOt3Z3P/mBTCQ4LcDscYEwCWVEy9KC1Tnl2STnJcBFOG2ABcxjRVllRMvVj47R62Hczj1+N6ERxkbztjmir7dJuA23Ygl+c+2Ur/zjFc2q+j2+EYYwIoYF2Kjdl99ATPLd3Kwm/3EBEazMypg2wALmOaOEsqxu8O5BTwl+XbeHvNblqIcNvw7twxMpm2EaFuh2aMCTBLKsZvfjxexJyV23ntX5mUlilThybyq4t60iE63O3QjDH1xJKKOWN5hSW88tlOXv5sB3lFJUwe1Jn7x6TQpV0rt0MzxtQzSyrmtBUUl/LmV7t4ccV2jh4vYvxZHXhwbC96dYxyOzRjjEssqZg6Ky4t4520LJ5fto39OQUM7xnLr8f1YlBia7dDM8a4zJKKqbWyMuWD7/Yy45Ot7DpygiFdWjPjmoGcnxzrdmjGmAbCkoqpkaqydPNBnl2Szpb9ufTuGMUrN6ZyUe/21kXYGHMSSyqmWv/KOMzTi9NZtzubpHateP7awUzoH0+LFpZMjDGnsqRifPr2hx+ZviSdLzKOEB8Tzh+n9OfKsxMIsUesGGOqYUnFnGTL/hyeXbKVTzYdoF1EKL+d0JfrhnWxpwobY2rFkooBIPPwcZ5bupVF6/cSGRrMr8emcPOF3YgMs7eIMab27Bujmdt3LJ/nl2WwIG03IUHCL0ckc8fI7rRuZY9UMcbUnSWVZupIXiGzV2zn9a92oapcP6wLd4/uQXt7pIox5gxYUmlmcgqKefmznbzy2Q7yi0uZMiSB+y7uSWJbe6SKMebMWVJpJvKLSnn9y0xmr9xO9oliLuvfkQfHptCjvT1SxRjjP5ZUmriikjLeTtvNX5Zt42BuISNT4pg2rhf9E2LcDs0Y0wRZUmmiSsuU/1u3h+eWbmX30XzOSWrDX64dzLDu7dwOzRjThFlSaWJUlcUb9/Pskq1sO5jHWZ2imXdzP0alxNkjVYwxAWdJpYlQVT7bdpjpS9L5LusY3eMimPVvQ7i0X0d7pIoxpt5YUmkC1u46ytMfp/P1zqN0bt2Sp68awJTBnQm2R6oYY+qZJZVGbOPeYzy7ZCvLtxwkNjKMxyeexdShiYQF2yNVjDHusKTSCO04lMeMT7by4Xf7iA4P5uFLenHT+Um0CrX/TmOMu+xbqBHZk53P80u38e43WYQGteCe0T24bUR3YlqGuB2aMcYAllQahcN5hcz6NIO3vvoBgBvO68pdo3oQFxXmcmTGGHMySyoN2LH8Yl5atYNXv9hJQXEpPz87kXvH9KRz65Zuh2aMMT5ZUmmAThSV8Nq/MpmzYjs5BSVMGBDPA2NTSI6LdDs0Y4ypliWVBqSwpJT5q3fzl+UZHM4r5KLe7fn1uBTO6mSPVDHGNA6WVBqAktIyFn67h5lLt7EnO5+h3doy5/ohpCa1dTs0Y4ypE0sqLiorUz7asJ8Zn6Sz/dBx+neO4akp/RneM9YeqWKMaZQsqbhAVVmx9RDTF6ezcW8OPdpHMuf6IYw/q6MlE2NMo2ZJpZ6t3nmUZxZvYU3mjyS2bcmzPx/IFYM7E2TP5zLGNAGWVOrJhj3HeGZxOiu3HqJ9VBhPXNGPa1ITCQ2253MZY5oOSyoBlnEwlxmfbOWf3++ndasQHr20Nzecl0TLUHs+lzGm6Qnoz2QRuURE0kUkQ0Qe8bF8koh8JyLrRCRNRC6stDxIRL4VkQ8rzf+Vs92NIvK01/xHnX2li8j4wB1ZzXYfPcG0d9Yz7rlVrEw/xL0X92TVw6P55chkSyjGmCYrYDUVEQkCZgFjgSxgjYgsUtVNXqstAxapqorIAGAB0Ntr+X3AZiDaa7ujgUnAAFUtFJH2zvy+wFTgLKATsFREUlS1NFDH6MvB3AJmLc/gr6t/QES45YJu3DkqmXaR9kgVY0zTF8jLX0OBDFXdASAi8/Ekg4qkoqp5XutHAFo+ISIJwOXAk8CDXuvdCfxRVQudbRx05k8C5jvzd4pIhhPDl34+Lp+yTxTxP6t2MO+LnRSXKlenJnLvxT2Ij7FHqhhjmo9AJpXOwG6v6SxgWOWVRGQy8BTQHk8SKTcTeBiIqlQkBRguIk8CBcA0VV3j7O+rSvvr7GN/twO3A3Tp0qVuR+TD8cISXv18J3NX7SCvqISJAzvxwJgUkmIjznjbxhjT2AQyqfjqI6unzFBdCCwUkRHAE8AYEZkAHFTVtSIyqlKRYKANcC5wDrBARLrXYX9zgbkAqamppyyvrYLiUv769Q/M+jSDI8eLGNOnA78el0Kf+OiaCxtjTBMVyKSSBSR6TScAe6taWVVXiUiyiMQCFwATReQyIByIFpE3VfV6Z7vvqaoCq0WkDIit6/7OxPrd2dz55lr2Hivg/OR2TBvfiyFd2gRiV8YY06gEsvfXGqCniHQTkVA8jeiLvFcQkR7i3EIuIkOAUOCIqj6qqgmqmuSUW+4kFID3gYucMilOmcPOtqeKSJiIdAN6AqsDcWBJ7SJIbh/JW7cO46+3nWsJxRhjHAGrqahqiYjcAywGgoBXVXWjiNzhLJ8DXAncICLFQD5wjVMDqc6rwKsisgEoAm50ymwUkQV4OgKUAHcHqudXTKsQ3vjFKc1DxhjT7EnN3+FNV2pqqqalpbkdhjHGNCoislZVU30ts2eEGGOM8RtLKsYYY/zGkooxxhi/saRijDHGbyypGGOM8RtLKsYYY/zGkooxxhi/adb3qYjIIWDXGWwiFs/d/A2NxVU3FlfdWFx10xTj6qqqcb4WNOukcqZEJK2qG4DcZHHVjcVVNxZX3TS3uOzylzHGGL+xpGKMMcZvLKmcmbluB1AFi6tuLK66sbjqplnFZW0qxhhj/MZqKsYYY/zGkooxxhi/saRSiYgkisinIrJZRDaKyH3O/J8702UiUmU3PBG5RETSRSRDRB5pQHFlisj3IrJORPw2iEw1cT0jIltE5DsRWSgirasoX9/nq7Zx1ff5esKJaZ2ILBGRTlWUr+/zVdu46vV8eS2fJiLqDEPuq3y9nq86xFXf76/ficgeZ3/rxDNUu6/yZ36+VNX+vP6AeGCI8zoK2Ar0BfoAvYAVQGoVZYOA7UB3PMMcrwf6uh2XUyYTiK3H8zUOCHbm/wn4UwM5XzXG5dL5ivZa515gTgM5XzXG5cb5cqYT8Ywsu8vXvt04X7WJy6X31++AaTWU9cv5sppKJaq6T1W/cV7nApuBzqq6WVXTayg+FMhQ1R2qWgTMByY1gLgCppq4lqhqibPaV0CCj+JunK/axBUw1cSV47VaBOCrB40b56s2cQVMVXE5i58DHq4mpno/X7WMK2BqiKsmfjlfllSqISJJwGDg61oW6Qzs9prOovb/obV2GnGB5w2+RETWisjt/o6phrhuAT7yUcTt81VVXODC+RKRJ0VkN3Ad8J8+irhyvmoRF9Tz+RKRicAeVV1fTZF6P1+1jAvc+Tze41zKfFVE2vgo4pfzZUmlCiISCfwduL/Sr7Vqi/mY59dfK6cZF8AFqjoEuBS4W0RG1EdcIvIboAR4y1cxH/Pq5XzVEBe4cL5U9TeqmujEdI+vYj7mBfx81SIuqMfzhef/7TdUneAqivmYF7DzVYe4oP7fX7OBZGAQsA941lcxH/PqfL4sqfggIiF4/kPeUtX36lA0C8/11HIJwN4GEBequtf59yCwEE9VN6BxiciNwATgOnUu2lbiyvmqRVyunC8vfwWu9DHf7fdXVXHV9/lKBroB60UkE895+EZEOlYqWt/nq7Zx1fv7S1UPqGqpqpYBL1WxP/+cL383FDX2PzzZ+nVgZhXLV1B1Q30wsAPPG6u8oeusBhBXBBDl9fpfwCWBjAu4BNgExFVTtt7PVy3jcuN89fR6/Svg3QZyvmoTV72fr0rrZOK7od61z2MNcbnx/or3ev0AMD9Q5+uMD6Kp/QEX4qnyfQesc/4uAybjyeSFwAFgsbN+J+CfXuUvw9PjYjvwm4YQF57eHOudv431FFcGnuuz5fPmNJDzVWNcLp2vvwMbnPkf4Gkkbwjnq8a43DhfldbJxPnydvt81SYul95fbwDfO/MX4SSZQJwve0yLMcYYv7E2FWOMMX5jScUYY4zfWFIxxhjjN5ZUjDHG+I0lFWOMMX5jScWYABORUq+nw67z89Nyk0Rkg7+2Z8yZCnY7AGOagXxVHeR2EMbUB6upGOMSZ0yNP4nIauevhzO/q4gscx7+t0xEujjzO4hnDJj1zt/5zqaCROQlZ/yMJSLS0rWDMs2eJRVjAq9lpctf13gty1HVocALwExn3gvA66o6AM9DHJ935j8PrFTVgcAQPHdjA/QEZqnqWUA2VTyfy5j6YHfUGxNgIpKnqpE+5mcCF6nqDuchgPtVtZ2IHMbzGI1iZ/4+VY0VkUNAgqoWem0jCfhEVXs60/8BhKjqHwJ/ZMacymoqxrhLq3hd1Tq+FHq9LsXaSo2LLKkY465rvP790nn9L2Cq8/o64HPn9TLgTgARCRKR6PoK0pjasl80xgReSxFZ5zX9saqWdysOE5Gv8fzAu9aZdy/wqog8BBwCbnbm3wfMFZFf4KmR3IlnwCVjGgxrUzHGJU6bSqqqHnY7FmP8xS5/GWOM8RurqRhjjPEbq6kYY4zxG0sqxhhj/MaSijHGGL+xpGKMMcZvLKkYY4zxm/8PERXf7woAJ+wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_epoch_array, val_bleu3_array, label = 'Validation BLEU3 score')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BLEU3 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(reference, hypothesis):\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-l'])\n",
    "    return evaluator.get_scores([hypothesis], [reference])['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_loader, vocab):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    rouge_scores = []\n",
    "    bleu1_scores = []\n",
    "    bleu2_scores = []\n",
    "    bleu3_scores = []\n",
    "    bleu4_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, semantic_inputs, captions, lengths) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            semantic_inputs = semantic_inputs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            caption = captions[0]\n",
    "            \n",
    "            image_feats = encoder(images)\n",
    "            image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "            \n",
    "            caption_list = [vocab.get_word(wid.item()) for wid in caption if wid != 0]                \n",
    "            caption_list = caption_list[1:-1]\n",
    "            caption_sentence = ' '.join(caption_list)\n",
    "            \n",
    "            output = model(image_feats, 12, semantic_inputs, None, beam=5)\n",
    "            output = output[1][1:-1] if output[1][-1] == end_token else output[1][1:]\n",
    "            output_sentence = ' '.join(output)\n",
    "            \n",
    "            print(\"Ref:\", caption_sentence)\n",
    "            print(\"Hyp:\", output_sentence)\n",
    "\n",
    "            rouge_scores.append(rouge_score(caption_sentence, output_sentence))\n",
    "            bleu1_scores.append(bleu_score(caption_list, output, 1))\n",
    "            bleu2_scores.append(bleu_score(caption_list, output, 2))\n",
    "            bleu3_scores.append(bleu_score(caption_list, output, 3))\n",
    "            try:\n",
    "                bleu4_scores.append(bleu_score(caption_list, output, 4))\n",
    "            except:\n",
    "                bleu4_scores.append(0)\n",
    "            \n",
    "#             if i % 500 == 0:\n",
    "#                 print(i)\n",
    "            if i == 2:\n",
    "                break\n",
    "            \n",
    "    rouge_score_avg = sum(rouge_scores)/len(rouge_scores)\n",
    "    bleu1_score_avg = sum(bleu1_scores)/len(bleu1_scores)\n",
    "    bleu2_score_avg = sum(bleu2_scores)/len(bleu2_scores)\n",
    "    bleu3_score_avg = sum(bleu3_scores)/len(bleu3_scores)\n",
    "    bleu4_score_avg = sum(bleu4_scores)/len(bleu4_scores)\n",
    "    \n",
    "    return rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref: lilac lustre nail polish\n",
      "Hyp: streetwear mauve nail polish\n",
      "Ref: women silk green kurta\n",
      "Hyp: fabindia women green kurta\n",
      "Ref: women freestyle sports analog steel black watch\n",
      "Hyp: fastrack women white dial watch\n"
     ]
    }
   ],
   "source": [
    "rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg = predict(caption_model, pred_loader, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5277777777777778 0.5060426728047519 0.30274943015462097 1.1065708905209323e-103 6.6928657911675046e-155\n"
     ]
    }
   ],
   "source": [
    "print(rouge_score_avg, bleu1_score_avg, bleu2_score_avg, bleu3_score_avg, bleu4_score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
