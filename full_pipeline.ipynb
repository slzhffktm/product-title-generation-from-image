{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import ast\n",
    "import heapq\n",
    "from copy import deepcopy\n",
    "from math import log\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "unknown_token = '<unk>'\n",
    "padding_token = '<pad>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor1d(tensor, split):\n",
    "    r\"\"\"Split 1D of tensor into N split\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "    Return\n",
    "        array of splitted tensor with N elements of array\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tensor[:split],\n",
    "        tensor[split: split * 2],\n",
    "        tensor[split * 2: split * 3],\n",
    "        tensor[split * 3:],\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_tensor2d(tensor, split, front=False):\n",
    "    r\"\"\"Split 2D of tensor into N split of 2D tensor\n",
    "\n",
    "    Arguments\n",
    "        tensor (Pytorch.Tensor) : tensor to split\n",
    "        split  (int) : number of split\n",
    "        front  (bool) : split axis 0 if True else axis 1\n",
    "    Return\n",
    "        array of splitted 2D tensor with N elements of array\n",
    "    \"\"\"\n",
    "\n",
    "    if front:\n",
    "        return [\n",
    "            tensor[:split, :],\n",
    "            tensor[split: split * 2, :],\n",
    "            tensor[split * 2: split * 3, :],\n",
    "            tensor[split * 3:, :],\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        tensor[:, :split],\n",
    "        tensor[:, split: split * 2],\n",
    "        tensor[:, split * 2: split * 3],\n",
    "        tensor[:, split * 3:],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexto1hot(vocab_len, index):\n",
    "    #print(\"index type: \")\n",
    "    if isinstance(index, int) == False:\n",
    "        n = len(index)\n",
    "        #print(\"making a 1hot encoding of shape: \" + str(n) + \",\" + str(vocab_len) )\n",
    "        one_hot = np.zeros([n,vocab_len])\n",
    "        #can this be optimized?\n",
    "        for i in range(n):\n",
    "            one_hot[i,index[i]]=1\n",
    "        \n",
    "        return one_hot\n",
    "    else:\n",
    "        one_hot = np.zeros([vocab_len])\n",
    "        one_hot[index] = 1\n",
    "        \n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zeros(shape, cuda=False):\n",
    "    zeros = torch.zeros(shape)\n",
    "    if cuda:\n",
    "        zeros = zeros.cuda()\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, dim_word_emb, dim_lang_lstm, dim_image_feats, nb_hidden):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(dim_lang_lstm+dim_image_feats+dim_word_emb,\n",
    "                                     nb_hidden,\n",
    "                                     bias=True)\n",
    "        \n",
    "    def forward(self, h1, c1, h2, v_mean, word_emb):\n",
    "        #print(h2.shape)\n",
    "        #print(v_mean.shape)\n",
    "        #print(word_emb.shape)\n",
    "        input_feats = torch.cat((h2, v_mean, word_emb),dim=1)\n",
    "        h_out, c_out = self.lstm_cell(input_feats, (h1, c1))\n",
    "        \n",
    "        return h_out, c_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    def __init__(self, dim_image_feats, dim_att_lstm, nb_hidden):\n",
    "        super(Attend, self).__init__()\n",
    "    \n",
    "        self.fc_image_feats = nn.Linear(dim_image_feats, nb_hidden, bias=False)\n",
    "        self.fc_att_lstm = nn.Linear(dim_att_lstm, nb_hidden, bias=False)\n",
    "        self.act_tan = nn.Tanh()\n",
    "        self.fc_att = nn.Linear(nb_hidden, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, image_feats, h1):\n",
    "        nb_batch, nb_feats, feat_dim = image_feats.size()\n",
    "        att_lstm_emb = self.fc_att_lstm(h1).unsqueeze(1)\n",
    "        image_feats_emb = self.fc_image_feats(image_feats)\n",
    "        all_feats_emb = image_feats_emb + att_lstm_emb.repeat(1,nb_feats,1)\n",
    "\n",
    "        activate_feats = self.act_tan(all_feats_emb)\n",
    "        unnorm_attention = self.fc_att(activate_feats)\n",
    "        normed_attention = self.softmax(unnorm_attention)\n",
    "\n",
    "        #print(normed_attention.shape)\n",
    "        #print(nb_feats)\n",
    "        #print(image_feats.shape)\n",
    "        #weighted_feats = normed_attention.repeat(1,1,nb_feats) * image_feats\n",
    "        weighted_feats = normed_attention * image_feats\n",
    "        #print(weighted_feats.shape)\n",
    "        attended_image_feats = weighted_feats.sum(dim=1)\n",
    "        #print(attended_image_feats.shape)\n",
    "    \n",
    "        return attended_image_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNCell(nn.Module):\n",
    "    r\"\"\"Custom Cell for Implementing Semantic Compositional Networks\n",
    "\n",
    "    Arguments\n",
    "        input_size (int): size of input\n",
    "        hidden_size (int): size of embedding\n",
    "        semantic_size (int): size of semantic\n",
    "        factor_size (int): size of factor\n",
    "        bias (boolean, optional): use bias?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, semantic_size, factor_size, bias=True):\n",
    "        super(SCNCell, self).__init__()\n",
    "\n",
    "        self.factor_size = factor_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.semantic_size = semantic_size\n",
    "\n",
    "        self.weight_ia = nn.Parameter(\n",
    "            torch.Tensor(input_size, 4 * factor_size))\n",
    "        self.weight_ib = nn.Parameter(\n",
    "            torch.Tensor(semantic_size, 4 * factor_size))\n",
    "        self.weight_ic = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "\n",
    "        self.weight_ha = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "        self.weight_hb = nn.Parameter(\n",
    "            torch.Tensor(semantic_size, 4 * factor_size))\n",
    "        self.weight_hc = nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 4 * factor_size))\n",
    "\n",
    "        if bias:\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, h1, v_hat, semantic_input, hx=None):\n",
    "        r\"\"\"Forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            wemb_input (torch.Tensor): word embedding input, a tensor of dimension (batch_size, input_size)\n",
    "            semantic_input (torch.Tensor): semantic concepts input, a tensor of dimension (batch_size, semantic_dim)\n",
    "        Returns\n",
    "            torch.Tensor: next hidden state, next cell state\n",
    "        \"\"\"\n",
    "\n",
    "        input_feats = torch.cat((h1, v_hat), dim=1)\n",
    "        \n",
    "        self.check_forward_input(input_feats)\n",
    "\n",
    "        [ia_i, ia_f, ia_o, ia_c] = split_tensor2d(\n",
    "            self.weight_ia, self.factor_size)\n",
    "        [ib_i, ib_f, ib_o, ib_c] = split_tensor2d(\n",
    "            self.weight_ib, self.factor_size)\n",
    "        [ic_i, ic_f, ic_o, ic_c] = split_tensor2d(\n",
    "            self.weight_ic, self.factor_size)\n",
    "        [b_ii, b_if, b_io, b_ic] = split_tensor1d(\n",
    "            self.bias_ih, self.hidden_size)\n",
    "\n",
    "        tmp1_i = (input_feats @ ia_i)\n",
    "        tmp1_f = (input_feats @ ia_f)\n",
    "        tmp1_o = (input_feats @ ia_o)\n",
    "        tmp1_c = (input_feats @ ia_c)\n",
    "\n",
    "        tmp2_i = (semantic_input @ ib_i).unsqueeze(0)\n",
    "        tmp2_f = (semantic_input @ ib_f).unsqueeze(0)\n",
    "        tmp2_o = (semantic_input @ ib_o).unsqueeze(0)\n",
    "        tmp2_c = (semantic_input @ ib_c).unsqueeze(0)\n",
    "\n",
    "        state_below_i = ((tmp1_i * tmp2_i) @ ic_i.t()) + b_ii\n",
    "        state_below_f = ((tmp1_f * tmp2_f) @ ic_f.t()) + b_if\n",
    "        state_below_o = ((tmp1_o * tmp2_o) @ ic_o.t()) + b_io\n",
    "        state_below_c = ((tmp1_c * tmp2_c) @ ic_c.t()) + b_ic\n",
    "\n",
    "        x_i = state_below_i.squeeze(0)\n",
    "        x_f = state_below_f.squeeze(0)\n",
    "        x_o = state_below_o.squeeze(0)\n",
    "        x_c = state_below_c.squeeze(0)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = input_feats.new_zeros(input_feats.size(\n",
    "                0), self.hidden_size, requires_grad=False)\n",
    "            hx = (hx, hx)\n",
    "\n",
    "        self.check_forward_hidden(x_i, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_i, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_f, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_f, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_o, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_o, hx[1], '[1]')\n",
    "\n",
    "        self.check_forward_hidden(x_c, hx[0], '[0]')\n",
    "        self.check_forward_hidden(x_c, hx[1], '[1]')\n",
    "\n",
    "        return self.recurrent_step(x_i, x_f, x_o, x_c, semantic_input, hx)\n",
    "\n",
    "    def recurrent_step(self, x_i, x_f, x_o, x_c, semantic_input, hx):\n",
    "        r\"\"\"Recurrent step helper for forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            x_i, x_f, x_o, x_c (torch.Tensor): factorized input, containing information from word embedding and semantic concepts, tensors of dimension (batch_size, input_size)\n",
    "            semantic_input (torch.Tensor): semantic concepts input, a tensor of dimension (batch_size, semantic_dim)\n",
    "            h_x (torch.Tensor): initial value of hidden and cell state\n",
    "        Returns\n",
    "            torch.Tensor: next hidden state, next cell state\n",
    "        \"\"\"\n",
    "\n",
    "        h_, c_ = hx\n",
    "\n",
    "        [ha_i, ha_f, ha_o, ha_c] = split_tensor2d(\n",
    "            self.weight_ha, self.factor_size)\n",
    "        [hb_i, hb_f, hb_o, hb_c] = split_tensor2d(\n",
    "            self.weight_hb, self.factor_size)\n",
    "        [hc_i, hc_f, hc_o, hc_c] = split_tensor2d(\n",
    "            self.weight_hc, self.factor_size)\n",
    "        [b_hi, b_hf, b_ho, b_hc] = split_tensor1d(\n",
    "            self.bias_hh, self.hidden_size)\n",
    "\n",
    "        preact_i = (h_ @ ha_i) * (semantic_input @ hb_i)\n",
    "        preact_i = (preact_i @ hc_i.t()) + x_i + b_hi\n",
    "\n",
    "        preact_f = (h_ @ ha_f) * (semantic_input @ hb_f)\n",
    "        preact_f = (preact_f @ hc_f.t()) + x_f + b_hf\n",
    "\n",
    "        preact_o = (h_ @ ha_o) * (semantic_input @ hb_o)\n",
    "        preact_o = (preact_o @ hc_o.t()) + x_o + b_ho\n",
    "\n",
    "        preact_c = (h_ @ ha_c) * (semantic_input @ hb_c)\n",
    "        preact_c = (preact_c @ hc_c.t()) + x_c + b_hc\n",
    "\n",
    "        i = torch.sigmoid(preact_i)\n",
    "        f = torch.sigmoid(preact_f)\n",
    "        o = torch.sigmoid(preact_o)\n",
    "        c = torch.tanh(preact_c)\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if 'bias' in self.__dict__ and self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
    "            s += ', nonlinearity={nonlinearity}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def check_forward_input(self, input):\n",
    "        if input.size(1) != self.input_size:\n",
    "            raise RuntimeError(\n",
    "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
    "                    input.size(1), self.input_size))\n",
    "\n",
    "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
    "        if input.size(0) != hx.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
    "                    input.size(0), hidden_label, hx.size(0)))\n",
    "\n",
    "        if hx.size(1) != self.hidden_size:\n",
    "            raise RuntimeError(\n",
    "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
    "                    hidden_label, hx.size(1), self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence is class for beam search\n",
    "class Sentence(object):\n",
    "    def __init__(self, max_nb_words, beam_width, end_word, vocab):\n",
    "        self.max_nb_words = max_nb_words\n",
    "        self.beam_width = beam_width\n",
    "        self.end_word = end_word\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.words = []\n",
    "        self.probability = 0\n",
    "        self.ended = False\n",
    "\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "\n",
    "    def update_words(self, s, state, y):\n",
    "        y = self.act(y)\n",
    "        new_s = []\n",
    "        for i in range(self.beam_width):\n",
    "            val, idx = y.max(dim=1)\n",
    "            y[0, idx] -= val\n",
    "            current_word = y.clone()\n",
    "            current_word[0,:] = 0\n",
    "            current_word[0,idx] = 1\n",
    "            s2 = s.copy()\n",
    "            s2.update_state(val, state, current_word)\n",
    "            new_s.append(s2)\n",
    "            if s2.ended:\n",
    "                break\n",
    "        return new_s\n",
    "\n",
    "    def update_state(self, p, state, current_word):\n",
    "        self.state = [s.clone() for s in state]\n",
    "        self.words.append(current_word)\n",
    "        self._update_probability(p)\n",
    "        self._update_finished()\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.state, self.words[-1]\n",
    "\n",
    "    def extract_sentence(self):\n",
    "        sentence = []\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            sentence.append(self.vocab.get_word(idx))\n",
    "        return [self.probability, sentence]\n",
    "\n",
    "    def _update_probability(self, p):\n",
    "        self.probability += log(p, 2)\n",
    "\n",
    "    def _update_finished(self):\n",
    "        n = len(self.words)\n",
    "        f = self.words[-1]\n",
    "        if (n > self.max_nb_words) or (f == self.end_word).all():\n",
    "            self.ended = True\n",
    "\n",
    "    def copy(self):\n",
    "        new = Sentence(self.max_nb_words,\n",
    "                       self.beam_width,\n",
    "                       self.end_word,\n",
    "                       self.vocab)\n",
    "        new.words = [w.clone() for w in self.words]\n",
    "        new.probability = self.probability\n",
    "        return new\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.probability < other.probability\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for w in self.words:\n",
    "            idx = w.max(1)[1].item()\n",
    "            s += \"{}, \".format(self.vocab.get_word(idx))\n",
    "        return s\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, beam_width):\n",
    "        self.beam_width = beam_width\n",
    "        self.heap = []\n",
    "\n",
    "    def push(self, s):\n",
    "        s.probability *= -1\n",
    "        heapq.heappush(self.heap, s)\n",
    "\n",
    "    def pop(self):\n",
    "        s = heapq.heappop(self.heap)\n",
    "        s.probability *= -1\n",
    "        return s\n",
    "\n",
    "    def trim(self):\n",
    "        h2 = []\n",
    "        for i in range(self.beam_width):\n",
    "            if len(self.heap) == 0:\n",
    "                break\n",
    "            heapq.heappush(h2, heapq.heappop(self.heap))\n",
    "        self.heap=h2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictWord(nn.Module):\n",
    "    def __init__(self, dim_language_lstm, dict_size):\n",
    "        super(PredictWord, self).__init__()\n",
    "        self.fc = nn.Linear(dim_language_lstm, dict_size)\n",
    "        \n",
    "    def forward(self, h2):\n",
    "        y = self.fc(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTagger(nn.Module):\n",
    "    r\"\"\"Tagger Encoder extends ResNet152 Model.\n",
    "\n",
    "    Arguments:\n",
    "        semantic_size (int, optional): size of semantic size\n",
    "        dropout (float, optional): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, semantic_size=1000, dropout=0.15):\n",
    "        super(ImageTagger, self).__init__()\n",
    "        self.semantic_size = semantic_size\n",
    "\n",
    "        # Using pre-trained ImageNet\n",
    "        resnet = torchvision.models.resnet152(pretrained=True)\n",
    "\n",
    "        # Remove linear layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(2048, semantic_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        r\"\"\"Forward propagation.\n",
    "\n",
    "        Arguments\n",
    "            images (torch.Tensor): images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        Returns \n",
    "            torch.Tensor: probabilites of tags (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.view(out.size(0), -1)   # (batch_size, 2048)\n",
    "        out = self.dropout(out)           # (batch_size, 2048)\n",
    "        out = self.linear(out)            # (batch_size, 1000)\n",
    "        out = self.sigmoid(out)           # (batch_size, 1000)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        r\"\"\"Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "        Arguments\n",
    "            fine_tune (boolean): Allow fine tuning?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dict_size, image_feature_dim, vocab, tf_ratio):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.dict_size = dict_size\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.vocab = vocab\n",
    "        self.tf_ratio = tf_ratio\n",
    "\n",
    "        self.embed_word = nn.Linear(dict_size, WORD_EMB_DIM, bias=False)\n",
    "        self.lstm1 = AttentionLSTM(WORD_EMB_DIM,\n",
    "                                   NB_HIDDEN_LSTM2,\n",
    "                                   image_feature_dim,\n",
    "                                   NB_HIDDEN_LSTM1)\n",
    "        self.lstm2 = SCNCell(NB_HIDDEN_LSTM1+image_feature_dim,\n",
    "                             NB_HIDDEN_LSTM2,\n",
    "                             SEMANTIC_DIM,\n",
    "                             FACTORED_DIM,\n",
    "                             bias=True)\n",
    "        self.attention = Attend(image_feature_dim,\n",
    "                                NB_HIDDEN_LSTM1,\n",
    "                                NB_HIDDEN_ATT)\n",
    "        self.predict_word = PredictWord(NB_HIDDEN_LSTM2, dict_size)\n",
    "\n",
    "        self.h1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.c1 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM1))\n",
    "        self.h2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "        self.c2 = torch.nn.Parameter(torch.zeros(1, NB_HIDDEN_LSTM2))\n",
    "    \n",
    "    def forward(self, image_feats, nb_timesteps, semantic_input, true_words, beam=None):\n",
    "        if beam is not None:\n",
    "            return self.beam_search(image_feats, nb_timesteps, beam, semantic_input)\n",
    "\n",
    "        nb_batch, nb_image_feats, _ = image_feats.size()\n",
    "        use_cuda = image_feats.is_cuda\n",
    "\n",
    "        v_mean = image_feats.mean(dim=1)\n",
    "\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "        y_out = make_zeros((nb_batch, nb_timesteps-1, self.dict_size), cuda = use_cuda)\n",
    "\n",
    "        for t in range(nb_timesteps-1):\n",
    "            y, state = self.forward_one_step(state,\n",
    "                                             current_word,\n",
    "                                             v_mean,\n",
    "                                             image_feats,\n",
    "                                             semantic_input)\n",
    "            y_out[:,t,:] = y\n",
    "\n",
    "            current_word = self.update_current_word(y, true_words, t, use_cuda)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "    def forward_one_step(self, state, current_word, v_mean, image_feats, semantic_input):\n",
    "        h1, c1, h2, c2 = state\n",
    "        word_emb = self.embed_word(current_word)\n",
    "        h1, c1 = self.lstm1(h1, c1, h2, v_mean, word_emb)\n",
    "        v_hat = self.attention(image_feats, h1)\n",
    "        h2, c2 = self.lstm2(h1, v_hat, semantic_input)\n",
    "        y = self.predict_word(h2)\n",
    "        state = [h1, c1, h2, c2]\n",
    "        \n",
    "        return y, state\n",
    "\n",
    "    def update_current_word(self, y, true_words, t, cuda):\n",
    "        use_tf = True if random.random() < self.tf_ratio else False\n",
    "        if use_tf:\n",
    "            next_word = true_words[:,t+1]\n",
    "        else:\n",
    "            next_word = torch.argmax(y, dim=1)\n",
    "\n",
    "        current_word = indexto1hot(len(self.vocab), next_word)\n",
    "        current_word = torch.from_numpy(current_word).float()\n",
    "        \n",
    "        if cuda:\n",
    "            current_word = current_word.cuda()\n",
    "    \n",
    "        return current_word\n",
    "\n",
    "    def init_inference(self, nb_batch, cuda):\n",
    "        start_word = indexto1hot(len(self.vocab), self.vocab('<start>'))\n",
    "        start_word = torch.from_numpy(start_word).float().unsqueeze(0)\n",
    "        start_word = start_word.repeat(nb_batch, 1)\n",
    "\n",
    "        if cuda:\n",
    "            start_word = start_word.cuda()\n",
    "\n",
    "        h1 = self.h1.repeat(nb_batch, 1)\n",
    "        c1 = self.c1.repeat(nb_batch, 1)\n",
    "        h2 = self.h2.repeat(nb_batch, 1)\n",
    "        c2 = self.c2.repeat(nb_batch, 1)\n",
    "        state = [h1, c1, h2, c2]\n",
    "\n",
    "        return state, start_word\n",
    "\n",
    "    #########################################\n",
    "    #               BEAM SEARCH             #\n",
    "    #########################################\n",
    "    def beam_search(self, image_features, max_nb_words, beam_width, semantic_input):\n",
    "        # Initialize model\n",
    "        use_cuda = image_features.is_cuda\n",
    "        nb_batch, nb_image_feats, _ = image_features.size()\n",
    "\n",
    "        v_mean = image_features.mean(dim=1)\n",
    "        state, current_word = self.init_inference(nb_batch, use_cuda)\n",
    "\n",
    "        # Initialize beam search\n",
    "        end_word = indexto1hot(len(self.vocab), self.vocab('<end>'))\n",
    "        end_word = torch.from_numpy(end_word).float().unsqueeze(0)\n",
    "        end_word = end_word.cuda() if image_features.is_cuda else end_word\n",
    "        beam = Beam(beam_width)\n",
    "        s = Sentence(max_nb_words, beam_width, end_word, self.vocab)\n",
    "        s.update_state(1.0, state, current_word)\n",
    "        beam.push(s)\n",
    "\n",
    "        # Perform beam search\n",
    "        final_beam = Beam(beam_width)\n",
    "        while len(beam) > 0:\n",
    "            s = beam.pop()\n",
    "            new_sentences = self.update_states(s, image_features, v_mean, semantic_input)\n",
    "            for s in new_sentences:\n",
    "                if s.ended:\n",
    "                    final_beam.push(s)\n",
    "                else:\n",
    "                    beam.push(s)\n",
    "            # Get rid of low scoring sentences\n",
    "            beam.trim() \n",
    "            final_beam.trim()\n",
    "\n",
    "        # Extract final sentence\n",
    "        s = final_beam.pop() # Best sentence on top of heap\n",
    "        sentence = s.extract_sentence()\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def update_states(self, s, image_feats, v_mean, semantic_input):\n",
    "        state, current_word = s.get_states()\n",
    "        y, state = self.forward_one_step(state, current_word, v_mean, image_feats, semantic_input)\n",
    "        y = self.remove_consecutive_words(y, current_word)\n",
    "        new_sentences = s.update_words(s, state, y)\n",
    "        return new_sentences\n",
    "\n",
    "    def remove_consecutive_words(self, y, prev_word):\n",
    "        # give previous word low score so as not to repeat words\n",
    "        y = y - 10**10 * prev_word\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_list):\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "    vocab.add_word(start_token)\n",
    "    vocab.add_word(end_token)\n",
    "    vocab.add_word(unknown_token)\n",
    "    vocab.add_word(padding_token)\n",
    "    \n",
    "    for word in word_list:\n",
    "        vocab.add_word(word)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_word_list = pd.read_csv(\"data/word_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab(df_word_list.word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMB_DIM = 1000\n",
    "NB_HIDDEN_LSTM1 = 1000\n",
    "NB_HIDDEN_LSTM2 = 1000\n",
    "NB_HIDDEN_ATT = 512\n",
    "\n",
    "IMAGE_FEATURE_DIM = 2048\n",
    "FACTORED_DIM = 1024\n",
    "SEMANTIC_DIM = 1000\n",
    "\n",
    "TF_RATIO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image_tagger = ImageTagger()\n",
    "# image_tagger = image_tagger.cuda()\n",
    "# image_tagger.load_state_dict(torch.load(\"models/image_tagger.best_1\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = image_tagger.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caption_model = CaptionModel(len(vocab), IMAGE_FEATURE_DIM, vocab, TF_RATIO)\n",
    "# caption_model = caption_model.cuda()\n",
    "# caption_model.load_state_dict(torch.load(\"models/caption.best_bleu3_1\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_transformer = transforms.Compose([\n",
    "#     transforms.Resize(224),\n",
    "#     transforms.RandomResizedCrop(224, scale=(1.0, 1.0)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(caption_model, image_tagger, encoder, vocab, image_file, use_url=False):\n",
    "\n",
    "    image_transformer = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomResizedCrop(224, scale=(1.0, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    caption_model.eval()\n",
    "    image_tagger.eval()\n",
    "\n",
    "    if use_url:\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    \n",
    "    display(image)\n",
    "    \n",
    "#     plt.imshow(image)\n",
    "    image = image_transformer(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        semantic_input = image_tagger(image)\n",
    "        image_feats = encoder(image)\n",
    "        image_feats = image_feats.unsqueeze(1).squeeze(3).squeeze(3)\n",
    "        output = caption_model(image_feats, 12, semantic_input, None, beam=5)\n",
    "        \n",
    "    output = output[1][1:-1] if output[1][-1] == end_token else output[1][1:]\n",
    "\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getter for Importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "\n",
    "    df_word_list = pd.read_csv(\"data/word_list.csv\")\n",
    "    vocab = build_vocab(df_word_list.word_list)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(vocab):\n",
    "    image_tagger = ImageTagger()\n",
    "    image_tagger = image_tagger.cuda()\n",
    "    image_tagger.load_state_dict(torch.load(\"models/image_tagger.best_1\")['model_state_dict'])\n",
    "    \n",
    "    encoder = image_tagger.resnet\n",
    "    \n",
    "    caption_model = CaptionModel(len(vocab), IMAGE_FEATURE_DIM, vocab, TF_RATIO)\n",
    "    caption_model = caption_model.cuda()\n",
    "    caption_model.load_state_dict(torch.load(\"models/caption.best_bleu3__best\")['model_state_dict'])\n",
    "    \n",
    "    return image_tagger, encoder, caption_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_io():\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    vocab = get_vocab()\n",
    "    image_tagger, encoder, caption_model = get_models(vocab)\n",
    "    \n",
    "    use_url = bool(input(\"Use url for input? (Leave blank for False)\"))\n",
    "\n",
    "    image_file = input(\"\\nInsert image url/file path: (Leave blank to exit)\")\n",
    "\n",
    "    while image_file != \"\":\n",
    "        print(\"Generated caption:\", predict(caption_model, image_tagger, encoder, vocab, image_file, use_url=use_url))\n",
    "\n",
    "        image_file = input(\"\\nInsert image url/file path: (Leave blank to exit)\")\n",
    "    \n",
    "    print(\"\\nThank you :)\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
